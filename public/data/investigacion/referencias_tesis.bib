@article{abbasianardakaniOpenaccessBreastLesion2023,
  title = {An Open-Access Breast Lesion Ultrasound Image Database‏: {{Applicable}} in Artificial Intelligence Studies},
  shorttitle = {An Open-Access Breast Lesion Ultrasound Image Database‏},
  author = {Abbasian Ardakani, Ali and Mohammadi, Afshin and Mirza-Aghazadeh-Attari, Mohammad and Acharya, U Rajendra},
  date = {2023-01-01},
  journaltitle = {Computers in Biology and Medicine},
  shortjournal = {Computers in Biology and Medicine},
  volume = {152},
  pages = {106438},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2022.106438},
  url = {https://www.sciencedirect.com/science/article/pii/S0010482522011465},
  urldate = {2025-08-10},
  abstract = {Breast cancer is one of the largest single contributors to the burden of disease worldwide. Early detection of breast cancer has been shown to be associated with better overall clinical outcomes. Ultrasonography is a vital imaging modality in managing breast lesions. In addition, the development of computer-aided diagnosis (CAD) systems has further enhanced the importance of this imaging modality. Proper development of robust and reproducible CAD systems depends on the inclusion of different data from different populations and centers to considerate all variations in breast cancer pathology and minimize confounding factors. The current database contains ultrasound images and radiologist-defined masks of two sets of histologically proven benign and malignant lesions. Using this and similar pieces of data can aid in the development of robust CAD systems.},
  keywords = {Artificial intelligence,Breast cancer,Deep learning,Diagnostics,Machine learning,Radiology,Segmentation,Ultrasound},
  file = {/home/caribu/Zotero/storage/RJE6NQNU/S0010482522011465.html}
}

@book{aggarwalNeuralNetworksDeep2023,
  title = {Neural Networks and Deep Learning: A Textbook},
  shorttitle = {Neural Networks and Deep Learning},
  author = {Aggarwal, Charu C.},
  date = {2023},
  edition = {Second Edition},
  publisher = {Springer International Publishing AG},
  location = {Cham},
  abstract = {This textbook covers both classical and modern models in deep learning and includes examples and exercises throughout the chapters. Deep learning methods for various data domains, such as text, images, and graphs are presented in detail. The chapters of this book span three categories: The basics of neural networks: The backpropagation algorithm is discussed in Chapter 2. Many traditional machine learning models can be understood as special cases of neural networks. Chapter 3 explores the connections between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 4 and 5. Chapters 6 and 7 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 8, 9, and 10 discuss recurrent neural networks, convolutional neural networks, and graph neural networks. Several advanced topics like deep reinforcement learning, attention mechanisms, transformer networks, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 11 and 12. The textbook is written for graduate students and upper under graduate level students. Researchers and practitioners working within this related field will want to purchase this as well. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques. The second edition is substantially reorganized and expanded with separate chapters on backpropagation and graph neural networks. Many chapters have been significantly revised over the first edition. Greater focus is placed on modern deep learning ideas such as attention mechanisms, transformers, and pre-trained language models},
  isbn = {978-3-031-29641-3},
  langid = {english},
  pagetotal = {529}
}

@online{ahmedEnhancingBreastCancer2024,
  title = {Enhancing {{Breast Cancer Diagnosis}} in {{Mammography}}: {{Evaluation}} and {{Integration}} of {{Convolutional Neural Networks}} and {{Explainable AI}}},
  shorttitle = {Enhancing {{Breast Cancer Diagnosis}} in {{Mammography}}},
  author = {Ahmed, Maryam and Bibi, Tooba and Khan, Rizwan Ahmed and Nasir, Sidra},
  date = {2024-04-27},
  eprint = {2404.03892},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2404.03892},
  urldate = {2024-05-15},
  abstract = {The Deep learning (DL) models for diagnosing breast cancer from mammographic images often operate as ”black boxes,” making it difficult for healthcare professionals to trust and understand their decision-making processes. The study presents an integrated framework combining Convolutional Neural Networks (CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an elaborate data preprocessing pipeline and advanced data augmentation techniques to counteract dataset limitations and transfer learning using pre-trained networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of our study is the evaluation of XAI’s effectiveness in interpreting model predictions, highlighted by utilising the Hausdorff measure to assess the alignment between AI-generated explanations and expert annotations quantitatively. This approach is critical for XAI in promoting trustworthiness and ethical fairness in AI-assisted diagnostics. The findings from our research illustrate the effective collaboration between CNNs and XAI in advancing diagnostic methods for breast cancer, thereby facilitating a more seamless integration of advanced AI technologies within clinical settings. By enhancing the interpretability of AIdriven decisions, this work lays the groundwork for improved collaboration between AI systems and medical practitioners, ultimately enriching patient care. Furthermore, the implications of our research extended well beyond the current methodologies. It encourages further research into how to combine multimodal data and improve AI explanations to meet the needs of clinical practice.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/caribu/Zotero/storage/BHXG82LP/Ahmed et al. - 2024 - Enhancing Breast Cancer Diagnosis in Mammography .pdf}
}

@article{al-dhabyaniDatasetBreastUltrasound2020,
  title = {Dataset of Breast Ultrasound Images},
  author = {Al-Dhabyani, Walid and Gomaa, Mohammed and Khaled, Hussien and Fahmy, Aly},
  date = {2020-02-01},
  journaltitle = {Data in Brief},
  shortjournal = {Data in Brief},
  volume = {28},
  pages = {104863},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2019.104863},
  url = {https://www.sciencedirect.com/science/article/pii/S2352340919312181},
  urldate = {2025-08-10},
  abstract = {Breast cancer is one of the most common causes of death among women worldwide. Early detection helps in reducing the number of early deaths. The data presented in this article reviews the~medical images of breast cancer using ultrasound scan. Breast Ultrasound Dataset is categorized into three classes: normal, benign, and malignant images. Breast ultrasound images can produce great results in classification, detection, and segmentation of breast cancer when combined with machine learning.},
  keywords = {Breast cancer,Classification,Dataset,Deep learning,Detection,Medical images,Segmentation,Ultrasound},
  file = {/home/caribu/Zotero/storage/Z9A7B4M4/S2352340919312181.html}
}

@article{al-tamMultimodalBreastCancer2024,
  title = {Multimodal Breast Cancer Hybrid Explainable Computer-Aided Diagnosis Using Medical Mammograms and Ultrasound {{Images}}},
  author = {Al-Tam, Riyadh M. and Al-Hejri, Aymen M. and Alshamrani, Sultan S. and Al-antari, Mugahed A. and Narangale, Sachin M.},
  date = {2024-07-01},
  journaltitle = {Biocybernetics and Biomedical Engineering},
  shortjournal = {Biocybernetics and Biomedical Engineering},
  volume = {44},
  number = {3},
  pages = {731--758},
  issn = {0208-5216},
  doi = {10.1016/j.bbe.2024.08.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0208521624000603},
  urldate = {2024-10-09},
  abstract = {Breast cancer is a prevalent global disease where early detection is crucial for effective treatment and reducing mortality rates. To address this challenge, a novel Computer-Aided Diagnosis (CAD) framework leveraging Artificial Intelligence (AI) techniques has been developed. This framework integrates capabilities for the simultaneous detection and classification of breast lesions. The AI-based CAD framework is meticulously structured into two pipelines (Stage 1 and Stage 2). The first pipeline (Stage 1) focuses on detectable cases where lesions are identified during the detection task. The second pipeline (Stage 2) is dedicated to cases where lesions are not initially detected. Various experimental scenarios, including binary (benign vs. malignant) and multi-class classifications based on BI-RADS scores, were conducted for training and evaluation. Additionally, a verification and validation (V\&V) scenario was implemented to assess the reliability of the framework using unseen multimodal datasets for both binary and multi-class tasks. For the detection tasks, the recent AI detectors like YOLO (You Only Look Once) variants were fine-tuned and optimized to localize breast lesions. For classification tasks, hybrid AI models incorporating ensemble convolutional neural networks (CNNs) and the attention mechanism of Vision Transformers were proposed to enhance prediction performance. The proposed AI-based CAD framework was trained and evaluated using various multimodal ultrasound datasets (BUSI and US2) and mammogram datasets (MIAS, INbreast, real private mammograms, KAU-BCMD, and CBIS-DDSM), either individually or in merged forms. Visual t-SNE techniques were applied to visually harmonize data distributions across ultrasound and mammogram datasets for effective various datasets merging. To generate visually explainable heatmaps in both pipelines (stages 1 and 2), Grad-CAM was utilized. These heatmaps assisted in finalizing detected boxes, especially in stage 2 when the AI detector failed to automatically detect breast lesions. The highest evaluation metrics achieved for merged dataset (BUSI, INbreast, and MIAS) were 97.73\% accuracy and 97.27\% mAP50 in the first pipeline. In the second pipeline, the proposed CAD achieved 91.66\% accuracy with 95.65\% mAP50 on MIAS and 95.65\% accuracy with 96.10\% mAP50 on the merged dataset (INbreast and MIAS). Meanwhile, exceptional performance was demonstrated using BI-RADS scores, achieving 87.29\% accuracy, 91.68\% AUC, 86.72\% mAP50, and 64.75\% mAP50-95 on a combined dataset of INbreast and CBIS-DDSM. These results underscore the practical significance of the proposed CAD framework in automatically annotating suspected lesions for radiologists.},
  keywords = {BI-RAD scores,Breast cancer detection and classification,Ensemble transfer learning,Explainable heatmap,Hybrid CAD system},
  file = {/home/caribu/Zotero/storage/TF9T73HY/Al-Tam et al. - 2024 - Multimodal breast cancer hybrid explainable computer-aided diagnosis using medical mammograms and ul.pdf;/home/caribu/Zotero/storage/J57CRKAJ/S0208521624000603.html}
}

@article{aliExplainableArtificialIntelligence2023,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{What}} We Know and What Is Left to Attain {{Trustworthy Artificial Intelligence}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M. and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and Díaz-Rodríguez, Natalia and Herrera, Francisco},
  date = {2023-11-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {99},
  pages = {101805},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.101805},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
  urldate = {2023-10-30},
  abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.},
  keywords = {AI principles,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Interpretable machine learning,Post-hoc explainability,Trustworthy AI,XAI assessment},
  file = {/home/caribu/Zotero/storage/FDQHB4KJ/Ali et al. - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf;/home/caribu/Zotero/storage/DH3CJP57/S1566253523001148.html}
}

@article{alonzo-proulxReliabilityAutomatedBreast2015,
  title = {Reliability of {{Automated Breast Density Measurements}}},
  author = {Alonzo-Proulx, Olivier and Mawdsley, Gordon E. and Patrie, James T. and Yaffe, Martin J. and Harvey, Jennifer A.},
  date = {2015-05},
  journaltitle = {Radiology},
  volume = {275},
  number = {2},
  pages = {366--376},
  publisher = {Radiological Society of North America},
  issn = {0033-8419},
  doi = {10.1148/radiol.15141686},
  url = {https://pubs.rsna.org/doi/10.1148/radiol.15141686},
  urldate = {2025-08-11},
  abstract = {Purpose To estimate the reliability of a reference standard two-dimensional area-based method and three automated volumetric breast density measurements by using repeated measures. Materials and Methods Thirty women undergoing screening mammography consented to undergo a repeated left craniocaudal examination performed by a second technologist in this prospective institutional review board–approved HIPAA-compliant study. Breast density was measured by using an area-based method (Cumulus ABD) and three automated volumetric methods (CumulusV [University of Toronto], Volpara [version 1.4.5; Volpara Solutions, Wellington, New Zealand), and Quantra [version 2.0; Hologic, Danbury, Conn]). Discrepancy between the first and second breast density measurements (Δ1–2) was obtained for each algorithm by subtracting the second measurement from the first. The Δ1–2 values of each algorithm were then analyzed with a random-effects model to derive Bland-Altman–type limits of measurement agreement. Results Variability was higher for Cumulus ABD and CumulusV than for Volpara or Quantra. The within-breast density measurement standard deviations were 3.32\% (95\% confidence interval [CI]: 2.65, 4.44), 3.59\% (95\% CI: 2.86, 4.48), 0.99\% (95\% CI: 0.79, 1.33), and 1.64\% (95\% CI: 1.31, 1.39) for Cumulus ABD, CumulusV, Volpara, and Quantra, respectively. Although the mean discrepancy between repeat breast density measurements was not significantly different from zero for any of the algorithms, larger absolute breast density discrepancy (Δ1–2) values were associated with larger breast density values for Cumulus ABD and CumulusV but not for Volpara and Quantra. Conclusion Variability in a repeated measurement of breast density is lowest for Volpara and Quantra; these algorithms may be more suited to incorporation into a risk model. © RSNA, 2015}
}

@dataset{andrioleRSNAScreeningMammography2022,
  title = {{{RSNA Screening Mammography Breast}}  {{Cancer Detection Challenge}}},
  author = {Andriole, Katherine P. and Ball, Robyn and Chen, Yan and Frazer, Helen and Kitamura, Felipe and Kwok, Jackson and Lungren, Matthew and Mann, Ritse and Mongan, John and Partridge, George and Moy, Linda and Trivedi, Hari and Wang, Xin and Yao, Luyan and Zhang, Tianyu},
  date = {2022},
  publisher = {Kaggle},
  url = {https://www.kaggle.com/competitions/rsna-breast-cancer-detection},
  langid = {english}
}

@inproceedings{Ansel_PyTorch_2_Faster_2024,
  title = {{{PyTorch}} 2: {{Faster}} Machine Learning through Dynamic Python Bytecode Transformation and Graph Compilation},
  booktitle = {29th {{ACM}} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 ({{ASPLOS}} '24)},
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and family=Luk, given=CK, given-i=CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  date = {2024-04},
  publisher = {ACM},
  doi = {10.1145/3620665.3640366},
  url = {https://pytorch.org/assets/pytorch2-2.pdf}
}

@article{apleyVisualizingEffectsPredictor2020,
  title = {Visualizing the {{Effects}} of {{Predictor Variables}} in {{Black Box Supervised Learning Models}}},
  author = {Apley, Daniel W. and Zhu, Jingyu},
  date = {2020-09-01},
  journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  shortjournal = {J. R. Stat. Soc. Ser. B. Stat. Methodol.},
  volume = {82},
  number = {4},
  pages = {1059--1086},
  issn = {1369-7412},
  doi = {10.1111/rssb.12377},
  url = {https://doi.org/10.1111/rssb.12377},
  urldate = {2025-08-10},
  abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
  file = {/home/caribu/Zotero/storage/PPV8HX7N/Apley y Zhu - 2020 - Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.pdf;/home/caribu/Zotero/storage/9E9S2M2F/rssb.html}
}

@article{article,
  title = {The Digital Database for Screening Mammography},
  author = {Heath, M and Bowyer, Kevin and Kopans, D and Moore, R and Kegelmeyer, P},
  date = {2000-01},
  journaltitle = {Proceedings of the Fourth International Workshop on Digital Mammography},
  doi = {10.1007/978-94-011-5318-8_75},
  isbn = {978-94-010-6234-3}
}

@article{arunAssessingTrustworthinessSaliency2021,
  title = {Assessing the {{Trustworthiness}} of {{Saliency Maps}} for {{Localizing Abnormalities}} in {{Medical Imaging}}},
  author = {Arun, Nishanth and Gaw, Nathan and Singh, Praveer and Chang, Ken and Aggarwal, Mehak and Chen, Bryan and Hoebel, Katharina and Gupta, Sharut and Patel, Jay and Gidwani, Mishka and Adebayo, Julius and Li, Matthew D. and Kalpathy-Cramer, Jayashree},
  date = {2021-11-01},
  journaltitle = {Radiology: Artificial Intelligence},
  shortjournal = {Radiology: Artificial Intelligence},
  volume = {3},
  number = {6},
  pages = {e200267},
  issn = {2638-6100},
  doi = {10.1148/ryai.2021200267},
  url = {http://pubs.rsna.org/doi/10.1148/ryai.2021200267},
  urldate = {2024-08-08},
  abstract = {Purpose:{$\quad$} To evaluate the trustworthiness of saliency maps for abnormality localization in medical imaging. Materials and Methods:{$\quad$} Using two large publicly available radiology datasets (Society for Imaging Informatics in Medicine–American College of Radiology Pneumothorax Segmentation dataset and Radiological Society of North America Pneumonia Detection Challenge dataset), the performance of eight commonly used saliency map techniques were quantified in regard to (a) localization utility (segmentation and detection), (b) sensitivity to model weight randomization, (c) repeatability, and (d) reproducibility. Their performances versus baseline methods and localization network architectures were compared, using area under the precision-recall curve (AUPRC) and structural similarity index measure (SSIM) as metrics. Results:{$\quad$} All eight saliency map techniques failed at least one of the criteria and were inferior in performance compared with localization networks. For pneumothorax segmentation, the AUPRC ranged from 0.024 to 0.224, while a U-Net achieved a significantly superior AUPRC of 0.404 (P , .005). For pneumonia detection, the AUPRC ranged from 0.160 to 0.519, while a RetinaNet achieved a significantly superior AUPRC of 0.596 (P , .005). Five and two saliency methods (of eight) failed the model randomization test on the segmentation and detection datasets, respectively, suggesting that these methods are not sensitive to changes in model parameters. The repeatability and reproducibility of the majority of the saliency methods were worse than localization networks for both the segmentation and detection datasets. Conclusion:{$\quad$} The use of saliency maps in the high-risk domain of medical imaging warrants additional scrutiny and recommend that detection or segmentation models be used if localization is the desired output of the network.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/D8UEFGR7/Arun et al. - 2021 - Assessing the Trustworthiness of Saliency Maps for.pdf}
}

@article{azamMammographicMicrocalcificationsRisk2021,
  title = {Mammographic Microcalcifications and Risk of Breast Cancer},
  author = {Azam, Shadi and Eriksson, Mikael and Sjölander, Arvid and Gabrielson, Marike and Hellgren, Roxanna and Czene, Kamila and Hall, Per},
  date = {2021-08},
  journaltitle = {British Journal of Cancer},
  shortjournal = {Br J Cancer},
  volume = {125},
  number = {5},
  pages = {759--765},
  publisher = {Nature Publishing Group},
  issn = {1532-1827},
  doi = {10.1038/s41416-021-01459-x},
  url = {https://www.nature.com/articles/s41416-021-01459-x},
  urldate = {2024-08-01},
  abstract = {Mammographic microcalcifications are considered early signs of breast cancer (BC). We examined the association between microcalcification clusters and the risk of overall and subtype-specific BC. Furthermore, we studied how mammographic density (MD) influences the association between microcalcification clusters and BC risk.},
  langid = {english},
  keywords = {Breast cancer,Cancer epidemiology},
  file = {/home/caribu/Zotero/storage/IW9K8DP5/Azam et al. - 2021 - Mammographic microcalcifications and risk of breas.pdf}
}

@incollection{baraziMammographyBIRADS2023,
  title = {Mammography {{BI RADS Grading}}},
  booktitle = {{{StatPearls}} [{{Internet}}]},
  author = {Barazi, Hassana and Gunduru, Mounika},
  date = {2023-07-31},
  eprint = {30969638},
  eprinttype = {pubmed},
  publisher = {StatPearls Publishing},
  url = {https://www.ncbi.nlm.nih.gov/sites/books/NBK539816/},
  urldate = {2025-08-11},
  abstract = {The breast imaging reporting and data system (BI-RADS) is a system for the standardization of mammogram reports. Developed by the American College of Radiology in 1993, its goal is to provide information to referring clinicians and patients in a language that is clear, meaningful and standardized across facilities. Based on scientific data and developed by the world’s leaders in breast imaging, the BI-RADS system describes key mammographic findings and outlines appropriate follow-up and management.[1] The Mammography Quality Standards Act (MQSA) of 1997 stipulates that all mammograms in the United States must be reported using one of the BI-RADS assessment categories.[2]},
  langid = {english},
  file = {/home/caribu/Zotero/storage/ZIT285YA/NBK539816.html}
}

@unpublished{barnettIAIABLCasebasedInterpretable2021,
  title = {{{IAIA-BL}}: {{A Case-based Interpretable Deep Learning Model}} for {{Classification}} of {{Mass Lesions}} in {{Digital Mammography}}},
  shorttitle = {{{IAIA-BL}}},
  author = {Barnett, Alina Jade and Schwartz, Fides Regina and Tao, Chaofan and Chen, Chaofan and Ren, Yinhao and Lo, Joseph Y. and Rudin, Cynthia},
  date = {2021-03-23},
  eprint = {2103.12308},
  eprinttype = {arXiv},
  eprintclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2103.12308},
  urldate = {2022-05-24},
  abstract = {Interpretability in machine learning models is important in high-stakes decisions, such as whether to order a biopsy based on a mammographic exam. Mammography poses important challenges that are not present in other computer vision tasks: datasets are small, confounding information is present, and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone. In this work, we present a framework for interpretable machine learning-based mammography. In addition to predicting whether a lesion is malignant or benign, our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image, such as the characteristics of the mass margins. The framework includes a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations, leading to better accuracy and interpretability even with a small number of images. Our interpretable models are able to highlight the classification-relevant parts of the image, whereas other methods highlight healthy tissue and confounding information. Our models are decision aids, rather than decision makers, aimed at better overall human-machine collaboration. We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,I.2.6,I.4.9},
  file = {/home/caribu/Zotero/storage/XZVGJB8W/Barnett et al. - 2021 - IAIA-BL A Case-based Interpretable Deep Learning .pdf;/home/caribu/Zotero/storage/4HWVDVAI/2103.html}
}

@article{bodewesMammographicBreastDensity2022,
  title = {Mammographic Breast Density and the Risk of Breast Cancer: {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {Mammographic Breast Density and the Risk of Breast Cancer},
  author = {Bodewes, F. T. H. and family=Asselt, given=A. A., prefix=van, useprefix=true and Dorrius, M. D. and Greuter, M. J. W. and family=Bock, given=G. H., prefix=de, useprefix=true},
  date = {2022-12-01},
  journaltitle = {The Breast},
  shortjournal = {The Breast},
  volume = {66},
  pages = {62--68},
  issn = {0960-9776},
  doi = {10.1016/j.breast.2022.09.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0960977622001618},
  urldate = {2025-08-01},
  abstract = {Objectives Mammographic density is a well-defined risk factor for breast cancer and having extremely dense breast tissue is associated with a one-to six-fold increased risk of breast cancer. However, it is questioned whether this increased risk estimate is applicable to current breast density classification methods. Therefore, the aim of this study was to further investigate and clarify the association between mammographic density and breast cancer risk based on current literature. Methods Medline, Embase and Web of Science were systematically searched for articles published since 2013, that used BI-RADS lexicon 5th edition and incorporated data on digital mammography. Crude and maximally confounder-adjusted data were pooled in odds ratios (ORs) using random-effects models. Heterogeneity regarding breast cancer risks were investigated using I2 statistic, stratified and sensitivity analyses. Results Nine observational studies were included. Having extremely dense breast tissue (BI-RADS density D) resulted in a 2.11-fold (95\% CI 1.84–2.42) increased breast cancer risk compared to having scattered dense breast tissue (BI-RADS density B). Sensitivity analysis showed that when only using data that had adjusted for age and BMI, the breast cancer risk was 1.83-fold (95\% CI 1.52–2.21) increased. Both results were statistically significant and homogenous. Conclusions Mammographic breast density BI-RADS D is associated with an approximately two-fold increased risk of breast cancer compared to having BI-RADS density B in general population women. This is a novel and lower risk estimate compared to previously reported and might be explained due to the use of digital mammography and BI-RADS lexicon 5th edition.},
  keywords = {BI-RADS,Breast cancer,Breast cancer risk,Breast density,Mammographic density},
  file = {/home/caribu/Zotero/storage/HX4R6TCC/Bodewes et al. - 2022 - Mammographic breast density and the risk of breast cancer A systematic review and meta-analysis.pdf}
}

@article{brownBreastCancerDense2023,
  title = {Breast {{Cancer}} in {{Dense Breasts}}: {{Detection Challenges}} and {{Supplemental Screening Opportunities}}},
  shorttitle = {Breast {{Cancer}} in {{Dense Breasts}}},
  author = {Brown, Ann L. and Vijapura, Charmi and Patel, Mitva and De La Cruz, Alexis and Wahab, Rifat},
  date = {2023-10-01},
  journaltitle = {RadioGraphics},
  shortjournal = {RadioGraphics},
  volume = {43},
  number = {10},
  pages = {e230024},
  issn = {0271-5333, 1527-1323},
  doi = {10.1148/rg.230024},
  url = {http://pubs.rsna.org/doi/10.1148/rg.230024},
  urldate = {2025-08-02},
  langid = {english},
  file = {/home/caribu/Zotero/storage/W9IEPQGW/Brown et al. - 2023 - Breast Cancer in Dense Breasts Detection Challenges and Supplemental Screening Opportunities.pdf}
}

@article{calistoIntroductionHumancentricAI2021,
  title = {Introduction of Human-Centric {{AI}} Assistant to Aid Radiologists for Multimodal Breast Image Classification},
  author = {Calisto, Francisco Maria and Santiago, Carlos and Nunes, Nuno and Nascimento, Jacinto C.},
  date = {2021-06},
  journaltitle = {International Journal of Human-Computer Studies},
  shortjournal = {International Journal of Human-Computer Studies},
  volume = {150},
  pages = {102607},
  issn = {10715819},
  doi = {10.1016/j.ijhcs.2021.102607},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581921000252},
  urldate = {2021-06-16},
  abstract = {In this research, we take an HCI perspective on the opportunities provided by AI techniques in medical imaging, focusing on workflow efficiency and quality, preventing errors and variability of diagnosis in Breast Cancer. Starting from a holistic understanding of the clinical context, we developed BreastScreening to support Multi­ modality and integrate AI techniques (using a deep neural network to support automatic and reliable classifica­ tion) in the medical diagnosis workflow. This was assessed by using a significant number of clinical settings and radiologists. Here we present: i) user study findings of 45 physicians comprising nine clinical institutions; ii) list of design recommendations for visualization to support breast screening radiomics; iii) evaluation results of a proof-of-concept BreastScreening prototype for two conditions Current (without AI assistant) and AI-Assisted; and iv) evidence from the impact of a Multimodality and AI-Assisted strategy in diagnosing and severity classification of lesions. The above strategies will allow us to conclude about the behaviour of clinicians when an AI module is present in a diagnostic system. This behaviour will have a direct impact in the clinicians workflow that is thor­ oughly addressed herein. Our results show a high level of acceptance of AI techniques from radiologists and point to a significant reduction of cognitive workload and improvement in diagnosis execution.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/Y59MSEMJ/Calisto et al. - 2021 - Introduction of human-centric AI assistant to aid .pdf}
}

@article{campanaPerceivedBarriersReaching2025,
  title = {Perceived Barriers to Reaching Equity in Effective Access to Diagnosis and Treatment for Women with Breast Cancer in {{Chile}}},
  author = {Campaña, Carla and Cabieses, Báltica and Obach, Alexandra and Vezzani, Francisca},
  date = {2025-02-05},
  journaltitle = {Humanities and Social Sciences Communications},
  shortjournal = {Humanit Soc Sci Commun},
  volume = {12},
  number = {1},
  pages = {143},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-024-04259-9},
  url = {https://www.nature.com/articles/s41599-024-04259-9},
  urldate = {2025-08-01},
  abstract = {Globally, it has been reported inequities in breast cancer effective access to health care. The objective of this study was to explore perceived inequities in access to effective diagnosis and treatment in women with breast cancer according to Tanahashi model and social determinants of health model. An exploratory case study, under a qualitative paradigm, was conducted. Theoretical sampling guided the selection of diverse participant profiles, comprising breast cancer patients, healthcare professionals and a civil society leader. The strategies for the recruitment process included social networks, civil society organizations, health professionals, and the snowball technique. Online semi-structured interviews were conducted. Interviews were transcribed, anonymized, and coded using ATLAS.Ti for deductive thematic analysis. Barriers to effective healthcare access were identified in all components of Tanahashi model. Accessibility and acceptability were the components with most perceived barriers. From the determinants of health model, a woman from the public health system, with low income, under 30 or over 40, and residing in a different region from the metropolitan region faces more barriers to access to an effective healthcare. The main barriers were for the high centralisation of healthcare in Chile, not integrated health system network, misinformation to the patient, and non-humanized healthcare. The results of this study offer a comprehensive exploration of perceived barriers to effective breast cancer diagnosis and treatment in Chile, using a qualitative approach incorporating diverse perspectives. Findings underscore significant systemic challenges across Tanahashi’s model components, impacting the overall care experience. The study reveals structural inequities hindering healthcare access, reflecting global patterns in fragmented health systems.},
  langid = {english},
  keywords = {Health humanities,Medical humanities},
  file = {/home/caribu/Zotero/storage/MQT85LKP/Campaña et al. - 2025 - Perceived barriers to reaching equity in effective access to diagnosis and treatment for women with.pdf}
}

@article{caoBreastMassDetection2021,
  title = {Breast Mass Detection in Digital Mammography Based on Anchor-Free Architecture},
  author = {Cao, Haichao and Pu, Shiliang and Tan, Wenming and Tong, Junyan},
  date = {2021-06-01},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  shortjournal = {Computer Methods and Programs in Biomedicine},
  volume = {205},
  pages = {106033},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2021.106033},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260721001085},
  urldate = {2023-01-05},
  abstract = {Background and objective Accurate detection of breast masses in mammography images is critical to diagnose early breast cancer, which can greatly improve the patients’ survival rate. However, it is still a big challenge due to the heterogeneity of breast masses and the complexity of their surrounding environment. Therefore, how to develop a robust breast mass detection framework in clinical practical applications to improve patient survival is a topic that researchers need to continue to explore. Methods To address these problems, we propose a one-stage object detection architecture, called Breast Mass Detection Network (BMassDNet), based on anchor-free and feature pyramid which makes the detection of breast masses of different sizes well adapted. We introduce a truncation normalization method and combine it with adaptive histogram equalization to enhance the contrast between the breast mass and the surrounding environment. Meanwhile, to solve the overfitting problem caused by small data size, we propose a natural deformation data augmentation method and mend the train data dynamic updating method based on the data complexity to effectively utilize the limited data. Finally, we use transfer learning to assist the training process and to improve the robustness of the model ulteriorly. Results On the INbreast dataset, each image has an average of 0.495 false positives whilst the recall rate is 0.930; On the DDSM dataset, when each image has 0.599 false positives, the recall rate reaches 0.943. Conclusions The experimental results on datasets INbreast and DDSM show that the proposed BMassDNet can obtain competitive detection performance over the current top ranked methods.},
  langid = {english},
  keywords = {Anchor-free architecture,aumentacion,breast cancer,Breast mass detection,CLAHE,Data augmentation method,Image enhancement method,Training method},
  file = {/home/caribu/Zotero/storage/YVEKK9WQ/Cao et al. - 2021 - Breast mass detection in digital mammography based.pdf;/home/caribu/Zotero/storage/PKM5GLDM/S0169260721001085.html}
}

@article{carterEthicalLegalSocial2020,
  title = {The Ethical, Legal and Social Implications of Using Artificial Intelligence Systems in Breast Cancer Care},
  author = {Carter, Stacy M. and Rogers, Wendy and Win, Khin Than and Frazer, Helen and Richards, Bernadette and Houssami, Nehmat},
  date = {2020-02-01},
  journaltitle = {The Breast},
  shortjournal = {The Breast},
  volume = {49},
  pages = {25--32},
  issn = {0960-9776},
  doi = {10.1016/j.breast.2019.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0960977619305648},
  urldate = {2021-06-17},
  abstract = {Breast cancer care is a leading area for development of artificial intelligence (AI), with applications including screening and diagnosis, risk calculation, prognostication and clinical decision-support, management planning, and precision medicine. We review the ethical, legal and social implications of these developments. We consider the values encoded in algorithms, the need to evaluate outcomes, and issues of bias and transferability, data ownership, confidentiality and consent, and legal, moral and professional responsibility. We consider potential effects for patients, including on trust in healthcare, and provide some social science explanations for the apparent rush to implement AI solutions. We conclude by anticipating future directions for AI in breast cancer care. Stakeholders in healthcare AI should acknowledge that their enterprise is an ethical, legal and social challenge, not just a technical challenge. Taking these challenges seriously will require broad engagement, imposition of conditions on implementation, and pre-emptive systems of oversight to ensure that development does not run ahead of evaluation and deliberation. Once artificial intelligence becomes institutionalised, it may be difficult to reverse: a proactive role for government, regulators and professional groups will help ensure introduction in robust research contexts, and the development of a sound evidence base regarding real-world effectiveness. Detailed public discussion is required to consider what kind of AI is acceptable rather than simply accepting what is offered, thus optimising outcomes for health systems, professionals, society and those receiving care.},
  langid = {english},
  keywords = {AI (Artificial Intelligence),Breast carcinoma,Ethical Issues,Social values,Technology Assessment Biomedical},
  file = {/home/caribu/Zotero/storage/P8TBD5CF/Carter et al. - 2020 - The ethical, legal and social implications of usin.pdf;/home/caribu/Zotero/storage/A73BNR67/S0960977619305648.html}
}

@article{castilloResultadosTratamientoCancer2017,
  title = {Resultados Del Tratamiento Del Cáncer de Mama, {{Programa Nacional}} de {{Cáncer}} Del {{Adulto}}},
  author = {Castillo, César del SM and Cabrera, M. Elena C. and Derio P., Lea and Gaete V., Fancy and Cavada CH., Gabriel and Castillo, César del SM and Cabrera, M. Elena C. and Derio P., Lea and Gaete V., Fancy and Cavada CH., Gabriel},
  date = {2017-12},
  journaltitle = {Revista médica de Chile},
  volume = {145},
  number = {12},
  pages = {1507--1513},
  publisher = {Sociedad Médica de Santiago},
  issn = {0034-9887},
  doi = {10.4067/s0034-98872017001201507},
  url = {https://scielo.conicyt.cl/scielo.php?script=sci_abstract&pid=S0034-98872017001201507&lng=es&nrm=iso&tlng=en},
  urldate = {2021-04-21},
  file = {/home/caribu/Zotero/storage/TEJZVUH8/Castillo et al. - 2017 - Resultados del tratamiento del cáncer de mama, Pro.pdf;/home/caribu/Zotero/storage/3NYXSF5X/scielo.html}
}

@article{cerekciQuantitativeEvaluationSaliencyBased2024,
  title = {Quantitative Evaluation of {{Saliency-Based Explainable}} Artificial Intelligence ({{XAI}}) Methods in {{Deep Learning-Based}} Mammogram Analysis},
  author = {Cerekci, Esma and Alis, Deniz and Denizoglu, Nurper and Camurdan, Ozden and Ege Seker, Mustafa and Ozer, Caner and Hansu, Muhammed Yusuf and Tanyel, Toygar and Oksuz, Ilkay and Karaarslan, Ercan},
  date = {2024-04-01},
  journaltitle = {European Journal of Radiology},
  shortjournal = {European Journal of Radiology},
  volume = {173},
  pages = {111356},
  issn = {0720-048X},
  doi = {10.1016/j.ejrad.2024.111356},
  url = {https://www.sciencedirect.com/science/article/pii/S0720048X2400072X},
  urldate = {2024-05-15},
  abstract = {Background Explainable Artificial Intelligence (XAI) is prominent in the diagnostics of opaque deep learning (DL) models, especially in medical imaging. Saliency methods are commonly used, yet there's a lack of quantitative evidence regarding their performance. Objectives To quantitatively evaluate the performance of widely utilized saliency XAI methods in the task of breast cancer detection on mammograms. Methods Three radiologists drew ground-truth boxes on a balanced mammogram dataset of women (n~=~1496 cancer-positive and negative scans) from three centers. A modified, pre-trained DL model was employed for breast cancer detection, using MLO and CC images. Saliency XAI methods, including Gradient-weighted Class Activation Mapping (Grad-CAM), Grad-CAM++, and Eigen-CAM, were evaluated. We utilized the Pointing Game to assess these methods, determining if the maximum value of a saliency map aligned with the bounding boxes, representing the ratio of correctly identified lesions among all cancer patients, with a value ranging from 0 to 1. Results The development sample included 2,244 women (75\%), with the remaining 748 women (25\%) in the testing set for unbiased XAI evaluation. The model's recall, precision, accuracy, and F1-Score in identifying cancer in the testing set were 69\%, 88\%, 80\%, and 0.77, respectively. The Pointing Game Scores for Grad-CAM, Grad-CAM++, and Eigen-CAM were 0.41, 0.30, and 0.35 in women with cancer and marginally increased to 0.41, 0.31, and 0.36 when considering only true-positive samples. Conclusions While saliency-based methods provide some degree of explainability, they frequently fall short in delineating how DL models arrive at decisions in a considerable number of instances.},
  keywords = {Breast Cancer,Deep Learning,Mammogram,XAI},
  file = {/home/caribu/Zotero/storage/T8HKEYXG/Cerekci et al. - 2024 - Quantitative evaluation of Saliency-Based Explaina.pdf;/home/caribu/Zotero/storage/W67BHNFM/S0720048X2400072X.html}
}

@inproceedings{chattopadhyayGradCAMImprovedVisual2018,
  title = {Grad-{{CAM}}++: {{Improved Visual Explanations}} for {{Deep Convolutional Networks}}},
  shorttitle = {Grad-{{CAM}}++},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
  date = {2018-03},
  eprint = {1710.11063},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {839--847},
  doi = {10.1109/WACV.2018.00097},
  url = {http://arxiv.org/abs/1710.11063},
  urldate = {2024-10-10},
  abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as ”black box” methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/Q4X2AERF/Chattopadhyay et al. - 2018 - Grad-CAM++ Improved Visual Explanations for Deep Convolutional Networks.pdf}
}

@incollection{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  volume = {11211},
  pages = {833--851},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01234-2_49},
  url = {https://link.springer.com/10.1007/978-3-030-01234-2_49},
  urldate = {2025-07-04},
  abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: //github.com/tensorflow/models/tree/master/research/deeplab.},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  langid = {english},
  file = {/home/caribu/Zotero/storage/9VKWNFCZ/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.pdf}
}

@inproceedings{ChenThisLooksLikeThat2019,
  title = {This Looks like That: {{Deep}} Learning for Interpretable Image Recognition},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Buc, given=F., prefix=dAlché-, useprefix=true and Fox, E. and Garnett, R.},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf},
  file = {/home/caribu/Zotero/storage/JU3S9PLP/Chen et al. - 2019 - This looks like that Deep learning for interpreta.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016-08-13},
  eprint = {1603.02754},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  url = {http://arxiv.org/abs/1603.02754},
  urldate = {2025-07-23},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  keywords = {Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/X3RJPY7R/Chen y Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;/home/caribu/Zotero/storage/IUVYPRSR/1603.html}
}

@article{costaDiagnosticDelaysBreast2024,
  title = {Diagnostic Delays in Breast Cancer among Young Women: {{An}} Emphasis on Healthcare Providers},
  shorttitle = {Diagnostic Delays in Breast Cancer among Young Women},
  author = {Costa, Luis and Kumar, Rakesh and Villarreal-Garza, Cynthia and Sinha, Saket and Saini, Sunil and Semwal, Jayanti and Saxsena, Vartika and Zamre, Vaishali and Chintamani, Chintamani and Ray, Mukurdipi and Shimizu, Chikako and Gusic, Lejla Hadzikadic and Toi, Masakazu and Lipton, Allan},
  date = {2024-02-01},
  journaltitle = {The Breast},
  shortjournal = {The Breast},
  volume = {73},
  pages = {103623},
  issn = {0960-9776},
  doi = {10.1016/j.breast.2023.103623},
  url = {https://www.sciencedirect.com/science/article/pii/S096097762300749X},
  urldate = {2025-07-31},
  abstract = {Despite advances in breast cancer care, breast cancer in young women (BCYW) faces unique challenges, diagnostic delays, and limited awareness in many countries. Here, we discuss the challenges and consequences associated with the delayed diagnosis of BCYW. The consequences of delayed diagnosis in young women - which generally varies among developed, developing, or underdeveloped countries - are severe due to a faster breast tumor growth rate than tumors in older women, also contributing to advanced cancer stages and poorer outcomes. Though there are many underlying reasons for diagnostic delays due to age, the article delves explicitly deep into the diagnostic delay of BCYW, focusing on healthcare providers, potential contributing factors, its consequences, and the urgent need to start minimizing such incidences. The article suggests several strategies to address these issues, including increasing awareness, developing educational programs for healthcare providers to identify signs and symptoms in young women, developing clear diagnostic guidelines, and improving screening strategies.},
  keywords = {Awareness,BCYW foundation,Breast cancer in young women,Diagnostic delay,Early detection,Healthcare providers,Tumor growth},
  file = {/home/caribu/Zotero/storage/7N4J5NJL/Costa et al. - 2024 - Diagnostic delays in breast cancer among young women An emphasis on healthcare providers.pdf;/home/caribu/Zotero/storage/JEHVW53Z/S096097762300749X.html}
}

@incollection{coughlinEpidemiologyBreastCancer2019,
  title = {Epidemiology of {{Breast Cancer}} in {{Women}}},
  booktitle = {Breast {{Cancer Metastasis}} and {{Drug Resistance}}: {{Challenges}} and {{Progress}}},
  author = {Coughlin, Steven S.},
  editor = {Ahmad, Aamir},
  date = {2019},
  series = {Advances in {{Experimental Medicine}} and {{Biology}}},
  pages = {9--29},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-20301-6_2},
  url = {https://doi.org/10.1007/978-3-030-20301-6_2},
  urldate = {2021-07-10},
  abstract = {Epidemiologic studies have contributed importantly to current knowledge of environmental and genetic risk factors for breast cancer. Worldwide, breast cancer is an important cause of human suffering and premature mortality among women. In the United States, breast cancer accounts for more cancer deaths in women than any site other than lung cancer. A variety of risk factors for breast cancer have been well-established by epidemiologic studies including race, ethnicity, family history of cancer, and genetic traits, as well as modifiable exposures such as increased alcohol consumption, physical inactivity, exogenous hormones, and certain female reproductive factors. Younger age at menarche, parity, and older age at first full-term pregnancy may influence breast cancer risk through long-term effects on sex hormone levels or by other biological mechanisms. Recent studies have suggested that triple negative breast cancers may have a distinct etiology. Genetic variants and mutations in genes that code for proteins having a role in DNA repair pathways and the homologous recombination of DNA double stranded breaks (APEX1, BRCA1, BRCA2, XRCC2, XRCC3, ATM, CHEK2, PALB2, RAD51, XPD), have been implicated in some cases of breast cancer.},
  isbn = {978-3-030-20301-6},
  langid = {english},
  keywords = {Alcohol,Breast cancer,Diet,Epidemiology,Genetics,Physical activity}
}

@dataset{cuiChineseMammographyDatabase2021,
  title = {The {{Chinese Mammography Database}} ({{CMMD}}): {{An}} Online Mammography Database with Biopsy Confirmed Types for Machine Diagnosis of Breast},
  shorttitle = {The {{Chinese Mammography Database}} ({{CMMD}})},
  author = {Cui, Chunyan and Li, Li and Cai, Hongmin and Fan, Zhihao and Zhang, Ling and Dan, Tingting and Li, Jiao and Wang, Jinghua},
  date = {2021},
  publisher = {The Cancer Imaging Archive},
  doi = {10.7937/TCIA.EQDE-4B16},
  url = {https://www.cancerimagingarchive.net/collection/cmmd/},
  urldate = {2025-08-10},
  abstract = {Breast carcinoma is the second largest cancer in the world among women. Early detection of breast cancer has been shown to increase the survival rate, thereby significantly increasing patients' lifespans. Mammography, a noninvasive imaging tool with low cost, is widely used to diagnose breast disease at an early stage due to its high sensitivity. The recent popularization of artificial intelligence in computer-aided diagnosis creates opportunities for advances in areas such as (1) Computer-aided detection for locating suspect lesions such as mass and microcalcification, leaving the classification to the radiologist; and (2) Computer-aided diagnosis for characterizing the suspicious region of lesion and/or estimate its probability of onset; and (3) Findings of predictive image-based biomarkers by applying the computational methods to mine the potential relationships between image representation and molecular subtype, including Luminal A, Luminal B, HER2 positive, and Triple-negative. However, existing publicly available mammography databases are limited by small sample size, lack of diversity in patient populations, missing biopsy confirmations and unknown molecular sub-types.  To help fill the gap, we built a database conducted on 1,775 patients from China with benign or malignant breast disease who underwent mammography examination between July 2012 and January 2016. The database consists of 3,728 mammographies from these 1,775 patients, with biopsy confirmed type of benign or malignant tumors. For 749 of these patients (1,498 mammographies) we also include patients' molecular subtypes. Image data were acquired on a GE Senographe DS mammography system.},
  version = {1}
}

@article{dahlTwostageMammographyClassification2023,
  title = {Two-Stage Mammography Classification Model Using Explainable-{{AI}} for {{ROI}} Detection},
  author = {Dahl, Fredrik and Brautaset, Olav and Holden, Marit and Eikvil, Line and Larsen, Marthe and Hofvind, Solveig},
  date = {2023-11-17},
  journaltitle = {Nordic Machine Intelligence},
  shortjournal = {NMI},
  volume = {3},
  number = {2},
  pages = {1--7},
  issn = {2703-9196},
  doi = {10.5617/nmi.10459},
  url = {https://journals.uio.no/NMI/article/view/10459},
  urldate = {2024-05-15},
  abstract = {This study introduces an enhanced version of a two-stage modelling approach using artificial intelligence (AI) for breast cancer detection in mammography screening. Leveraging a large dataset of 2,863,175 mammograms from the BreastScreen Norway, the approach uses two convolutional neural networks. The first one is trained to classify whole images, and an explainable-AI method is applied to this network to identify a region of interest (ROI). The second neural network subsequently classifies the ROI for malignancy. While a prior method used simple gradient saliency maps to identify ROIs, a key enhancement of the present methodology is the application of Layered GradCam, which identifies cancerous areas more consistently and allows smaller ROIs. Layered GradCam is also used to display identified cancers to the user. By the AUC criterion, our model performs well, 0.974 for screen-detected and 0.931 for all cancers (screen-detected and interval), compared to a commercial program; 0.959 and 0.918, respectively. Comparisons with the radiologist scores indicate that the model has equal performance with two radiologists, and superior performance to one, for the detection of all cancers (screening- and interval type). Our tests indicate that our model generalizes well for different breast centers, but so far only images from a single manufacturer have been tested.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/DVA8C5GX/Dahl et al. - 2023 - Two-stage mammography classification model using e.pdf}
}

@article{dembrowerMultimillionMammographyImage2020,
  title = {A {{Multi-million Mammography Image Dataset}} and {{Population-Based Screening Cohort}} for the {{Training}} and {{Evaluation}} of {{Deep Neural Networks}}—the {{Cohort}} of {{Screen-Aged Women}} ({{CSAW}})},
  author = {Dembrower, Karin and Lindholm, Peter and Strand, Fredrik},
  date = {2020-04},
  journaltitle = {Journal of Digital Imaging},
  shortjournal = {J Digit Imaging},
  volume = {33},
  number = {2},
  eprint = {31520277},
  eprinttype = {pubmed},
  pages = {408--413},
  issn = {0897-1889},
  doi = {10.1007/s10278-019-00278-0},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7165146/},
  urldate = {2025-08-09},
  abstract = {For AI researchers, access to a large and well-curated dataset is crucial. Working in the field of breast radiology, our aim was to develop a high-quality platform that can be used for evaluation of networks aiming to predict breast cancer risk, estimate mammographic sensitivity, and detect tumors. Our dataset, Cohort of Screen-Aged Women (CSAW), is a population-based cohort of all women 40 to 74~years of age invited to screening in the Stockholm region, Sweden, between 2008 and 2015. All women were invited to mammography screening every 18 to 24~months free of charge. Images were collected from the PACS of the three breast centers that completely cover the region. DICOM metadata were collected together with the images. Screening decisions and clinical outcome data were collected by linkage to the regional cancer center registers. Incident cancer cases, from one center, were pixel-level annotated by a radiologist. A separate subset for efficient evaluation of external networks was defined for the uptake area of one center. The collection and use of the dataset for the purpose of AI research has been approved by the Ethical Review Board. CSAW included 499,807 women invited to screening between 2008 and 2015 with a total of 1,182,733 completed screening examinations. Around 2 million mammography images have currently been collected, including all images for women who developed breast cancer. There were 10,582 women diagnosed with breast cancer; for 8463, it was their first breast cancer. Clinical data include biopsy-verified breast cancer diagnoses, histological origin, tumor size, lymph node status, Elston grade, and receptor status. One thousand eight hundred ninety-one images of 898 women had tumors pixel level annotated including any tumor signs in the prior negative screening mammogram. Our dataset has already been used for evaluation by several research groups. We have defined a high-volume platform for training and evaluation of deep neural networks in the domain of mammographic imaging.},
  pmcid = {PMC7165146},
  file = {/home/caribu/Zotero/storage/SG25PAZM/Dembrower et al. - 2020 - A Multi-million Mammography Image Dataset and Population-Based Screening Cohort for the Training and.pdf}
}

@article{diaz-ramirezAprendizajeAutomaticoAprendizaje2021,
  title = {Aprendizaje {{Automático}} y {{Aprendizaje Profundo}}},
  author = {Díaz-Ramírez, Jorge},
  date = {2021},
  journaltitle = {Ingeniare. Revista chilena de ingeniería},
  volume = {29},
  number = {2},
  pages = {180--181},
  issn = {undefined},
  doi = {10.4067/s0718-33052021000200180},
  url = {https://www.mendeley.com/catalogue/d059627d-ecc9-38fd-b45c-bf85e898279b/},
  urldate = {2025-08-11},
  abstract = {(2021) Díaz-Ramírez. Ingeniare. Revista chilena de ingeniería. Actualmente hablar de Inteligencia Artificial (IA) es para algunos hablar de robots y películas de ciencia ficción tales como, Termina...},
  issue = {2},
  langid = {british},
  file = {/home/caribu/Zotero/storage/KU6DI9L9/Díaz-Ramírez - 2021 - Aprendizaje Automático y Aprendizaje Profundo.pdf;/home/caribu/Zotero/storage/YG82T925/d059627d-ecc9-38fd-b45c-bf85e898279b.html}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-07-29},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/Q7YB9TKQ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf}
}

@article{duNewMethodDetecting2019,
  title = {A {{New Method}} for {{Detecting Architectural Distortion}} in {{Mammograms}} by {{NonSubsampled Contourlet Transform}} and {{Improved PCNN}}},
  author = {Du, Guangming and Dong, Min and Sun, Yi and Li, Shuyi and Mu, Xiaomin and Wei, Hongbin and Ma, Lei and Liu, Bang},
  date = {2019-11-15},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {9},
  number = {22},
  pages = {4916},
  issn = {2076-3417},
  doi = {10.3390/app9224916},
  url = {https://www.mdpi.com/2076-3417/9/22/4916},
  urldate = {2021-08-06},
  abstract = {Breast cancer is the leading cause of cancer death in women, and early detection can reduce mortality. Architectural distortion (AD) is a feature of clinical manifestations for breast cancer, however, due to its complex structure and low detection accuracy, which cause a high mortality of breast cancer. In order to improve the accuracy of AD detection and reduce the mortality of breast cancer, this paper proposes a new method by combining the non-subsampled contourlet transform (NSCT) with the improved pulse coupled neural network (PCNN). Firstly, the top–bottom hat transformation and the exponential transformation are employed to enhance the image. Secondly, the NSCT is employed to expand the overall contrast of the mammograms and filter out the noise. Finally, the improved PCNN by the maximum inter-class variance threshold selection method is employed to complete the AD detection. This proposed approach is tested on the public and authoritative database—Digital Database for Screening Mammography (DDSM). The specificity of the method is 98.73\%, the accuracy is 93.16\%, and the F1-score is 79.80\%, and the area under curve (AUC) of the receiver operating characteristic (ROC) curve is 0.93, these results clearly demonstrate that the proposed method is comparable with those methods in recent literatures. This proposed method is simple, furthermore it can achieve high accuracy and help doctors to perform computer-aided detection of AD effectively.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/4K52ZRZ6/Du et al. - 2019 - A New Method for Detecting Architectural Distortio.pdf}
}

@article{erikssonClinicalModelIdentifying2017,
  title = {A Clinical Model for Identifying the Short-Term Risk of Breast Cancer},
  author = {Eriksson, Mikael and Czene, Kamila and Pawitan, Yudi and Leifland, Karin and Darabi, Hatef and Hall, Per},
  date = {2017-12},
  journaltitle = {Breast Cancer Research},
  shortjournal = {Breast Cancer Res},
  volume = {19},
  number = {1},
  pages = {29},
  issn = {1465-542X},
  doi = {10.1186/s13058-017-0820-y},
  url = {http://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-017-0820-y},
  urldate = {2024-08-01},
  abstract = {Background: Most mammography screening programs are not individualized. To efficiently screen for breast cancer, the individual risk of the disease should be determined. We describe a model that could be used at most mammography screening units without adding substantial cost. Methods: The study was based on the Karma cohort, which included 70,877 participants. Mammograms were collected up to 3 years following the baseline mammogram. A prediction protocol was developed using mammographic density, computer-aided detection of microcalcifications and masses, use of hormone replacement therapy (HRT), family history of breast cancer, menopausal status, age, and body mass index. Relative risks were calculated using conditional logistic regression. Absolute risks were calculated using the iCARE protocol. Results: Comparing women at highest and lowest mammographic density yielded a fivefold higher risk of breast cancer for women at highest density. When adding microcalcifications and masses to the model, high-risk women had a nearly ninefold higher risk of breast cancer than those at lowest risk. In the full model, taking HRT use, family history of breast cancer, and menopausal status into consideration, the AUC reached 0.71. Conclusions: Measures of mammographic features and information on HRT use, family history of breast cancer, and menopausal status enabled early identification of women within the mammography screening program at such a high risk of breast cancer that additional examinations are warranted. In contrast, women at low risk could probably be screened less intensively.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/WQ5G84PI/Eriksson et al. - 2017 - A clinical model for identifying the short-term ri.pdf}
}

@article{farragExplainableAISystem2023,
  title = {An {{Explainable AI System}} for {{Medical Image Segmentation With Preserved Local Resolution}}: {{Mammogram Tumor Segmentation}}},
  shorttitle = {An {{Explainable AI System}} for {{Medical Image Segmentation With Preserved Local Resolution}}},
  author = {Farrag, Aya and Gad, Gad and Fadlullah, Zubair Md. and Fouda, Mostafa M. and Alsabaan, Maazen},
  date = {2023},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {11},
  pages = {125543--125561},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3330465},
  url = {https://ieeexplore.ieee.org/document/10309920/},
  urldate = {2024-06-06},
  abstract = {Medical image segmentation aims to identify important or suspicious regions within medical images. However, many challenges are usually faced while developing networks for this type of analysis. First, preserving the original image resolution is of utmost importance for this task where identifying subtle features or abnormalities can significantly impact the accuracy of diagnosis. While introducing the dilated convolution improves the resolution of the convolutional neural network (CNN), it is not without shortcoming, i.e., the loss of local spatial resolution due to increased kernel sparsity in checkboard patterns. To address this shortcoming, we conceptualize a double-dilated convolution module for maintaining local spatial resolution while improving the receptive field size. Then, this approach is applied, as a proof-ofwork, to tumor segmentation task in mammograms. In addition, our proposal also tackles the class imbalance problem, originating at the pixel level of the mammogram screenings, by identifying and selecting the best candidate among a number of potential loss functions to facilitate mass segmentation. We also carry out quantitative and qualitative evaluations of the interpretability of our proposal by leveraging Grad-CAM (Gradient weighted Class Activation Map). We also present a comparative performance evaluation with existing explainable techniques tailored for segmenting images. Moreover, an empirical assessment on lesion segmentation is conducted on mammogram samples from the INBreast dataset, both with and without incorporating our envisaged dilation module into CNN. The obtained results elucidate the effectiveness of our proposal based on mass segmentation performance measures, such as Dice similarity and Miss Detection rate. Our analysis also promotes using the Tversky Loss function in training pixel-imbalanced data and integrating Grad-CAM for explaining image segmentation results.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/Y83NEJ2I/Farrag et al. - 2023 - An Explainable AI System for Medical Image Segment.pdf}
}

@article{ferlayGlobalCancerObservatory2020,
  title = {Global Cancer Observatory: Cancer Today},
  shorttitle = {Global Cancer Observatory},
  author = {Ferlay, Jacques and Ervik, M. and Lam, F. and Colombet, M. and Mery, L. and Piñeros, M. and Znaor, A. and Soerjomataram, I. and Bray, F.},
  date = {2020},
  journaltitle = {Lyon: International agency for research on cancer},
  volume = {20182020}
}

@article{frankDeepLearningArchitecture2023,
  title = {A Deep Learning Architecture with an Object-Detection Algorithm and a Convolutional Neural Network for Breast Mass Detection and Visualization},
  author = {Frank, Steven J.},
  date = {2023-11-01},
  journaltitle = {Healthcare Analytics},
  shortjournal = {Healthcare Analytics},
  volume = {3},
  pages = {100186},
  issn = {2772-4425},
  doi = {10.1016/j.health.2023.100186},
  url = {https://www.sciencedirect.com/science/article/pii/S2772442523000539},
  urldate = {2023-11-09},
  abstract = {This study presents an integrated deep learning architecture with an object-detection algorithm and a convolutional neural network (CNN) for breast mass detection and visualization. Mammograms are analyzed to identify and localize breast mass lesions to aid clinician review. Two complementary forms of deep learning are used to identify the regions of interest (ROIs). An object-detection algorithm, YOLO v5, analyzes the entire mammogram to identify discrete image regions likely to represent masses. Object detections exhibit high precision, but the object-detection stage alone has insufficient overall accuracy for a clinical application. A CNN independently analyzes the mammogram after it has been decomposed into subregion tiles and is trained to emphasize sensitivity (recall). The ROIs identified by each analysis are highlighted in different colors to facilitate an efficient staged review. The CNN stage nearly always detects tumor masses when present but typically occupies a larger area of the image. By inspecting the high-precision regions followed by the high-sensitivity regions, clinicians can quickly identify likely lesions before completing the review of the full mammogram. On average, the ROIs occupy less than 20\% of the tissue in the mammograms, even without removing pectoral muscle from the analysis. As a result, the proposed system helps clinicians review mammograms with greater accuracy and efficiency.},
  keywords = {Convolutional neural network,Decision support,Deep learning,Intelligence,Object detection,Radiology},
  file = {/home/caribu/Zotero/storage/NI9ZJPDG/Frank - 2023 - A deep learning architecture with an object-detection algorithm and a convolutional neural network f.pdf;/home/caribu/Zotero/storage/NWJ68T7G/S2772442523000539.html}
}

@article{gerbasiDeepMiCaAutomaticSegmentation2023,
  title = {{{DeepMiCa}}: {{Automatic}} Segmentation and Classification of Breast {{MIcroCAlcifications}} from Mammograms},
  shorttitle = {{{DeepMiCa}}},
  author = {Gerbasi, Alessia and Clementi, Greta and Corsi, Fabio and Albasini, Sara and Malovini, Alberto and Quaglini, Silvana and Bellazzi, Riccardo},
  date = {2023-06-01},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  shortjournal = {Computer Methods and Programs in Biomedicine},
  volume = {235},
  pages = {107483},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2023.107483},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260723001499},
  urldate = {2025-07-15},
  abstract = {Background and objective Breast cancer is the world’s most prevalent form of cancer. The survival rates have increased in the last years mainly due to factors such as screening programs for early detection, new insights on the disease mechanisms as well as personalised treatments. Microcalcifications are the only first detectable sign of breast cancer and diagnosis timing is strongly related to the chances of survival. Nevertheless microcalcifications detection and classification as benign or malignant lesions is still a challenging clinical task and their malignancy can only be proven after a biopsy procedure. We propose DeepMiCa, a fully automated and visually explainable deep-learning based pipeline for the analysis of raw mammograms with microcalcifications. Our aim is to propose a reliable decision support system able to guide the diagnosis and help the clinicians to better inspect borderline difficult cases. Methods DeepMiCa is composed by three main steps: (1) Preprocessing of the raw scans (2) Automatic patch-based Semantic Segmentation using a UNet based network with a custom loss function appositely designed to deal with extremely small lesions (3) Classification of the detected lesions with a deep transfer-learning approach. Finally, state-of-the-art explainable AI methods are used to produce maps for a visual interpretation of the classification results. Each step of DeepMiCa is designed to address the main limitations of the previous proposed works resulting in a novel automated and accurate pipeline easily customisable to meet radiologists’ needs. Results The proposed segmentation and classification algorithms achieve an area under the ROC curve of 0.95 and 0.89 respectively. Compared to previously proposed works, this method does not require high performance computational resources and provides a visual explanation of the final classification results. Conclusion To conclude, we designed a novel fully automated pipeline for detection and classification of breast microcalcifications. We believe that the proposed system has the potential to provide a second opinion in the diagnosis process giving the clinicians the opportunity to quickly visualise and inspect relevant imaging characteristics. In the clinical practice the proposed decision support system could help reduce the rate of misclassified lesions and consequently the number of unnecessary biopsies.},
  keywords = {Classification,Deep learning,Explainability,Mammograms,Microcalcifications,Segmentation},
  file = {/home/caribu/Zotero/storage/SXDG9J56/Gerbasi et al. - 2023 - DeepMiCa Automatic segmentation and classification of breast MIcroCAlcifications from mammograms.pdf;/home/caribu/Zotero/storage/VU3ZC6DA/S0169260723001499.html}
}

@online{ghaeiniSaliencyLearningTeaching2019,
  title = {Saliency {{Learning}}: {{Teaching}} the {{Model Where}} to {{Pay Attention}}},
  shorttitle = {Saliency {{Learning}}},
  author = {Ghaeini, Reza and Fern, Xiaoli Z. and Shahbazi, Hamed and Tadepalli, Prasad},
  date = {2019-04-04},
  eprint = {1902.08649},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.08649},
  url = {http://arxiv.org/abs/1902.08649},
  urldate = {2025-08-10},
  abstract = {Deep learning has emerged as a compelling solution to many NLP tasks with remarkable performances. However, due to their opacity, such models are hard to interpret and trust. Recent work on explaining deep models has introduced approaches to provide insights toward the model's behaviour and predictions, which are helpful for assessing the reliability of the model's predictions. However, such methods do not improve the model's reliability. In this paper, we aim to teach the model to make the right prediction for the right reason by providing explanation training and ensuring the alignment of the model's explanation with the ground truth explanation. Our experimental results on multiple tasks and datasets demonstrate the effectiveness of the proposed method, which produces more reliable predictions while delivering better results compared to traditionally trained models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/KRLGWRVC/Ghaeini et al. - 2019 - Saliency Learning Teaching the Model Where to Pay Attention.pdf;/home/caribu/Zotero/storage/S7DBPZZD/1902.html}
}

@article{goldsteinPeekingBlackBox2015,
  title = {Peeking {{Inside}} the {{Black Box}}: {{Visualizing Statistical Learning With Plots}} of {{Individual Conditional Expectation}}},
  shorttitle = {Peeking {{Inside}} the {{Black Box}}},
  author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  date = {2015-01-02},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {24},
  number = {1},
  pages = {44--65},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1080/10618600.2014.907095},
  url = {https://doi.org/10.1080/10618600.2014.907095},
  urldate = {2025-08-10},
  abstract = {This article presents individual conditional expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the PDP by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data-generating model. Through simulated examples and real datasets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
  keywords = {Exploratory data analysis,Graphical method,Model visualization},
  file = {/home/caribu/Zotero/storage/AGLFLZ3J/Goldstein et al. - 2015 - Peeking Inside the Black Box Visualizing Statistical Learning With Plots of Individual Conditional.pdf}
}

@article{gudheMultiViewDeepEvidential2024,
  title = {A {{Multi-View Deep Evidential Learning Approach}} for {{Mammogram Density Classification}}},
  author = {Gudhe, Naga Raju and Mazen, Sudah and Sund, Reijo and Kosma, Veli-Matti and Behravan, Hamid and Mannermaa, Arto},
  date = {2024},
  journaltitle = {IEEE Access},
  volume = {12},
  pages = {67889--67909},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3399204},
  url = {https://ieeexplore.ieee.org/document/10528291/?arnumber=10528291},
  urldate = {2024-12-17},
  abstract = {Artificial intelligence algorithms, specifically deep learning, can assist radiologists by automating mammogram density assessment. However, trust in such algorithms must be established before they are widely adopted in clinical settings. In this study, we present an evidential deep learning approach called MV-DEFEAT, incorporating the strength of Dempster Shafer evidential theory and subjective logic, for the mammogram density classification task. The framework combines evidence from multiple mammograms’ views to mimic a radiologist decision making process. In this study, we utilized four open-source datasets, namely VinDr-Mammo, DDSM, CMMD, and VTB, to mitigate inherent biases and provide a diverse representation of the data. Our experimental findings demonstrate MV-DEFEAT’s superior performance in terms of weighted macro-average area under the receiver operating curves (AUCs) compared to the state-of-the-art multi-view deep learning model, referred to as MVDL. MV-DEFEAT yields a relative improvement of 12.57\%, 14.51\%, 19.9\%, and 22.53\%, on the VTB, VinDr-Mammo, CMMD, and DDSM datasets, respectively, for the mammogram density classification task. Additionally, for BIRADS classification and the classification of mammograms as benign or malignant, MV-DEFEAT exhibits substantial enhancements compared to MVDL, with a relative improvement of 31.46\% and 50.78\% on the DDSM and VinDr-Mammo datasets, respectively. These results underscore the efficacy of our approach. Through meticulous curation of diverse datasets and comprehensive comparative analyses, we ensure the robustness and reliability of our findings, thereby enhancing trust to adopt MV-DEFEAT framework for various mammogram assessment tasks in clinical settings.},
  eventtitle = {{{IEEE Access}}},
  keywords = {BIRADS classification,Breast,Breast cancer,Computational modeling,deep learning,evidential learning,Feature extraction,malignancy,mammogram density,mammograms,Mammography,multi view analysis,Predictive models,single view analysis,Task analysis,Uncertainty},
  file = {/home/caribu/Zotero/storage/ADS8XJQS/Gudhe et al. - 2024 - A Multi-View Deep Evidential Learning Approach for Mammogram Density Classification.pdf;/home/caribu/Zotero/storage/UQYUCMHG/10528291.html}
}

@article{gunningDARPAsExplainableArtificial2019,
  title = {{{DARPA}}'s {{Explainable Artificial Intelligence Program}}},
  author = {Gunning, David and Aha, David W.},
  date = {2019},
  journaltitle = {AI Magazine},
  volume = {40},
  number = {2},
  pages = {44--58},
  issn = {2371-9621},
  doi = {10.1609/aimag.v40i2.2850},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v40i2.2850},
  urldate = {2024-11-07},
  abstract = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA's explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems' explanations improve user understanding, user trust, and user task performance.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/CTHBLVG9/AI Magazine - 2019 - Gunning - DARPA s Explainable Artificial Intelligence Program.pdf;/home/caribu/Zotero/storage/HJQX9QQA/Gunning y Aha - 2019 - DARPA's Explainable Artificial Intelligence Program.pdf;/home/caribu/Zotero/storage/HLI4I97H/aimag.v40i2.html}
}

@article{hamedAutomatedBreastCancer2021,
  title = {Automated {{Breast Cancer Detection}} and {{Classification}} in {{Full Field Digital Mammograms Using Two Full}} and {{Cropped Detection Paths Approach}}},
  author = {Hamed, Ghada and Marey, Mohammed and Amin, Safaa Elsayed and Tolba, Mohamed F.},
  date = {2021},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {9},
  pages = {116898--116913},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3105924},
  url = {https://ieeexplore.ieee.org/document/9515973/},
  urldate = {2024-02-06},
  abstract = {Breast cancer is one of the most severe diseases that threaten women’s life results in increasing the death rate annually as confirmed by the World Health Organization. Breast cancer early detection is one of the main reasons behind reducing cancer severity. However, with the huge number of mammograms taken daily, the checking process conducted by radiologists becomes lengthy, tiring, and pruning to errors process. Hence, with the tremendous success achieved by utilizing CNNs in bioinformatics, the development of Computer-Aided Detection (CAD) systems has proved its necessity to solve the challenging cases for the biopsies missed by the ordinary checking leads to decreasing the false positive and negative rates. In this paper, we present a YOLOV4 based CAD system to localize lesions in full and cropped mammograms and then classify them to obtain their pathology type. The proposed method mainly consists of three phases that are applied on the full-field digital mammograms of the INbreast dataset. First, the mammograms are preprocessed to remove any extra artifacts and then cropped into small, overlapped slices. Second, masses are localized through two paths: the full mammograms and the cropped slices detection after configuring the YOLO-V4 model. Third, other feature extractors like ResNet, VGG, Inception, etc. are used to classify the localized lesions to compare their performance against YOLO. The proposed method proved using the experimental results the impact of utilizing YOLO-V4 as a detector with the 2-paths of detection of a full mammogram and the cropped slices in a trial to avoid any data loss by resizing the large-sized mammograms. Our system succeeds in detecting the masses’ location with an overall accuracy of ≈98\% which is more than the recently introduced breast cancer detection methods. Moreover, its ability to distinguish between benign and malignant tumors with an accuracy of ≈95\%.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/CVJIYUTB/Hamed et al. - 2021 - Automated Breast Cancer Detection and Classificati.pdf}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2024-12-02},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/8QZHBHUC/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/caribu/Zotero/storage/X4N3KNLX/1512.html}
}

@online{hermanPromisePerilHuman2019,
  title = {The {{Promise}} and {{Peril}} of {{Human Evaluation}} for {{Model Interpretability}}},
  author = {Herman, Bernease},
  date = {2019-10-30},
  eprint = {1711.07414},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.07414},
  url = {http://arxiv.org/abs/1711.07414},
  urldate = {2025-08-10},
  abstract = {Transparency, user trust, and human comprehension are popular ethical motivations for interpretable machine learning. In support of these goals, researchers evaluate model explanation performance using humans and real world applications. This alone presents a challenge in many areas of artificial intelligence. In this position paper, we propose a distinction between descriptive and persuasive explanations. We discuss reasoning suggesting that functional interpretability may be correlated with cognitive function and user preferences. If this is indeed the case, evaluation and optimization using functional metrics could perpetuate implicit cognitive bias in explanations that threaten transparency. Finally, we propose two potential research directions to disambiguate cognitive function and explanation models, retaining control over the tradeoff between accuracy and interpretability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/JD5TB8FZ/1711.html}
}

@article{hoffmanMeasuresExplainableAI2023,
  title = {Measures for Explainable {{AI}}: {{Explanation}} Goodness, User Satisfaction, Mental Models, Curiosity, Trust, and Human-{{AI}} Performance},
  shorttitle = {Measures for Explainable {{AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  date = {2023},
  journaltitle = {Frontiers in Computer Science},
  volume = {5},
  issn = {2624-9898},
  url = {https://www.frontiersin.org/articles/10.3389/fcomp.2023.1096257},
  urldate = {2023-05-30},
  abstract = {If a user is presented an AI system that portends to explain how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? This question entails some key concepts of measurement such as explanation goodness and trust. We present methods for enabling developers and researchers to: (1) Assess the a priori goodness of explanations, (2) Assess users' satisfaction with explanations, (3) Reveal user's mental model of an AI system, (4) Assess user's curiosity or need for explanations, (5) Assess whether the user's trust and reliance on the AI are appropriate, and finally, (6) Assess how the human-XAI work system performs. The methods we present derive from our integration of extensive research literatures and our own psychometric evaluations. We point to the previous research that led to the measurement scales which we aggregated and tailored specifically for the XAI context. Scales are presented in sufficient detail to enable their use by XAI researchers. For Mental Model assessment and Work System Performance, XAI researchers have choices. We point to a number of methods, expressed in terms of methods' strengths and weaknesses, and pertinent measurement issues.},
  file = {/home/caribu/Zotero/storage/DRZMXIWG/Hoffman et al. - 2023 - Measures for explainable AI Explanation goodness,.pdf}
}

@unpublished{hoffmanMetricsExplainableAI2019,
  title = {Metrics for {{Explainable AI}}: {{Challenges}} and {{Prospects}}},
  shorttitle = {Metrics for {{Explainable AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  date = {2019-02-01},
  eprint = {1812.04608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.04608},
  urldate = {2021-12-21},
  abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/caribu/Zotero/storage/7L2TR3WL/Hoffman et al. - 2019 - Metrics for Explainable AI Challenges and Prospec.pdf;/home/caribu/Zotero/storage/77JPSIPI/1812.html}
}

@article{holowkoHeritabilityMammographicBreast2020,
  title = {Heritability of {{Mammographic Breast Density}}, {{Density Change}}, {{Microcalcifications}}, and {{Masses}}},
  author = {Holowko, Natalie and Eriksson, Mikael and Kuja-Halkola, Ralf and Azam, Shadi and He, Wei and Hall, Per and Czene, Kamila},
  date = {2020-04-02},
  journaltitle = {Cancer Research},
  shortjournal = {Cancer Res},
  volume = {80},
  number = {7},
  pages = {1590--1600},
  issn = {0008-5472},
  doi = {10.1158/0008-5472.CAN-19-2455},
  url = {https://doi.org/10.1158/0008-5472.CAN-19-2455},
  urldate = {2025-08-01},
  abstract = {Mammographic features influence breast cancer risk and are used in risk prediction models. Understanding how genetics influence mammographic features is important because the mechanisms through which they are associated with breast cancer are not well known. Here, using mammographic screening history and detailed questionnaire data from 56,820 women from the KARMA prospective cohort study, we investigated the association between a genetic predisposition to breast cancer and mammographic features among women with a family history of breast cancer (N = 49,674) and a polygenic risk score (PRS, N = 9,365). The heritability of mammographic features such as dense area (MD), microcalcifications, masses, and density change (MDC, cm2/year) was estimated using 1,940 sister pairs. Heritability was estimated at 58\% [95\% confidence interval (CI), 48\%–67\%) for MD, 23\% (2\%–45\%) for microcalcifications, and 13\% (1\%–25\%)] for masses. The estimated heritability for MDC was essentially null (2\%; 95\% CI, −8\% to 12\%). The association between a genetic predisposition to breast cancer (using PRS) and MD and microcalcifications was positive, while for masses this was borderline significant. In addition, for MDC, having a family history of breast cancer was associated with slightly greater MD reduction. In summary, we have confirmed previous findings of heritability in MD, and also established heritability of the number of microcalcifications and masses at baseline. Because these features are associated with breast cancer risk and can improve detecting women at short-term risk of breast cancer, further investigation of common loci associated with mammographic features is warranted to better understand the etiology of breast cancer.These findings provide novel data on the heritability of microcalcifications, masses, and density change, which are all associated with breast cancer risk and can indicate women at short-term risk.},
  file = {/home/caribu/Zotero/storage/4IYNIDJE/Holowko et al. - 2020 - Heritability of Mammographic Breast Density, Density Change, Microcalcifications, and Masses.pdf;/home/caribu/Zotero/storage/7AGPQ59R/Holowko et al. - 2020 - Heritability of Mammographic Breast Density, Density Change, Microcalcifications, and Masses.pdf;/home/caribu/Zotero/storage/MI29V5MR/0008-5472.html}
}

@article{holzingerExplainableAIMultiModal2020,
  title = {Explainable {{AI}} and {{Multi-Modal Causability}} in {{Medicine}}},
  author = {Holzinger, Andreas},
  date = {2020-12-01},
  journaltitle = {i-com},
  volume = {19},
  number = {3},
  pages = {171--179},
  publisher = {De Gruyter Oldenbourg},
  issn = {2196-6826},
  doi = {10.1515/icom-2020-0024},
  url = {https://www.degruyter.com/document/doi/10.1515/icom-2020-0024/html},
  urldate = {2021-06-09},
  abstract = {Progress in statistical machine learning made AI in medicine successful, in certain classification tasks even beyond human level performance. Nevertheless, correlation is not causation and successful models are often complex “black-boxes”, which make it hard to understand why a result has been achieved. The explainable AI (xAI) community develops methods, e.\,g. to highlight which input parameters are relevant for a result; however, in the medical domain there is a need for causability: In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations produced by xAI. The key for future human-AI interfaces is to map explainability with causability and to allow a domain expert to ask questions to understand why an AI came up with a result, and also to ask “what-if” questions (counterfactuals) to gain insight into the underlying independent explanatory factors of a result. A multi-modal causability is important in the medical domain because often different modalities contribute to a result.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/PWLI5V6Y/Holzinger - 2020 - Explainable AI and Multi-Modal Causability in Medi.pdf;/home/caribu/Zotero/storage/GE4MVZ7Y/html.html}
}

@article{hovelingDiagnosticDelayWomen2025,
  title = {Diagnostic Delay in Women with Cancer: {{What}} Do We Know and Which Factors Contribute?},
  shorttitle = {Diagnostic Delay in Women with Cancer},
  author = {Hoveling, Liza A. and Schuurman, Melinda and Siesling, Sabine and family=Asselt, given=Kristel M., prefix=van, useprefix=true and Bode, Christina},
  date = {2025-04-01},
  journaltitle = {The Breast},
  shortjournal = {The Breast},
  volume = {80},
  pages = {104427},
  issn = {0960-9776},
  doi = {10.1016/j.breast.2025.104427},
  url = {https://www.sciencedirect.com/science/article/pii/S0960977625000463},
  urldate = {2025-08-01},
  abstract = {Timely cancer diagnosis is important, but delays are common, also among women. This study reviews recent literature on diagnostic delays in women with breast cancer, focusing on individual-level factors and their interaction with micro, meso, exo, and macrosystem factors. Following PRISMA-ScR guidelines, we conducted a scoping review on diagnostic delays in cancer among women, including qualitative and quantitative studies with oncological patients or healthcare professionals. We searched PubMed/MEDLINE and Scopus for publications from 2018 to November 28, 2023, excluding studies not meeting the inclusion criteria, not in English or Dutch, or focused solely on cancer screening. Titles and full texts were screened, with disagreements resolved through discussion. Two reviewers independently extracted study details, population characteristics, study design, and factors contributing to diagnostic delays. Initially, 9699 records were retrieved, resulting in 129 relevant studies after exclusions. We focused on women's health and breast cancer, narrowing our scope to 22 studies in high-income countries. Studies explored diagnostic delays and factors at various levels: microsystem (demographics, health behaviours, psychology, healthcare interactions), mesosystem (schedules, peer and support networks), exosystem (social, cultural, environmental, accessibility factors), and macrosystem (broader cultural, societal contexts, healthcare policies). In high-income countries, diagnostic delays in breast cancer care involve factors across various systems, affecting individuals, peers, healthcare, and policies. Enhancing awareness, communication, and access is important, requiring targeted campaigns and infrastructure upgrades. The Bronfenbrenner's ecological model effectively addresses the multifaceted factors influencing diagnostic delays. Future research can benefit from applying this model to various cancers and income settings.},
  keywords = {Breast cancer,Bronfenbrenner's ecological model,Diagnostic delays,High-income countries,Individual-level factors},
  file = {/home/caribu/Zotero/storage/4PSGYAH8/Hoveling et al. - 2025 - Diagnostic delay in women with cancer What do we know and which factors contribute.pdf;/home/caribu/Zotero/storage/B9SR6U87/S0960977625000463.html}
}

@online{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017-04-16},
  eprint = {1704.04861},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.04861},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2023-11-10},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/BQUGSXG7/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/home/caribu/Zotero/storage/V6YKMLLH/1704.html}
}

@online{huangDenselyConnectedConvolutional2018,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and family=Maaten, given=Laurens, prefix=van der, useprefix=true and Weinberger, Kilian Q.},
  date = {2018-01-28},
  eprint = {1608.06993},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.06993},
  url = {http://arxiv.org/abs/1608.06993},
  urldate = {2023-11-10},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/ZUGS846Y/Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf;/home/caribu/Zotero/storage/9AGGIQMG/1608.html}
}

@online{huSqueezeandExcitationNetworks2019,
  title = {Squeeze-and-{{Excitation Networks}}},
  author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  date = {2019-05-16},
  eprint = {1709.01507},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1709.01507},
  url = {http://arxiv.org/abs/1709.01507},
  urldate = {2025-07-29},
  abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of ∼25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/YE7FU7I6/Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf}
}

@article{ibrokhimovTwoStageDeepLearning2022,
  title = {Two-{{Stage Deep Learning Method}} for {{Breast Cancer Detection Using High-Resolution Mammogram Images}}},
  author = {Ibrokhimov, Bunyodbek and Kang, Justin-Youngwook},
  date = {2022-01},
  journaltitle = {Applied Sciences},
  volume = {12},
  number = {9},
  pages = {4616},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app12094616},
  url = {https://www.mdpi.com/2076-3417/12/9/4616},
  urldate = {2025-08-06},
  abstract = {Breast cancer screening and detection using high-resolution mammographic images have always been a difficult task in computer vision due to the presence of very small yet clinically significant abnormal growths in breast masses. The size difference between such masses and the overall mammogram image as well as difficulty in distinguishing intra-class features of the Breast Imaging Reporting and Database System (BI-RADS) categories creates challenges for accurate diagnosis. To obtain near-optimal results, object detection models should be improved by directly focusing on breast cancer detection. In this work, we propose a new two-stage deep learning method. In the first stage, the breast area is extracted from the mammogram and small square patches are generated to narrow down the region of interest (RoI). In the second stage, breast masses are detected and classified into BI-RADS categories. To improve the classification accuracy for intra-classes, we design an effective tumor classification model and combine its results with the detection model’s classification scores. Experiments conducted on the newly collected high-resolution mammography dataset demonstrate our two-stage method outperforms the original Faster R-CNN model by improving mean average precision (mAP) from 0.85 to 0.94. In addition, comparisons with existing works on a popular INbreast dataset validate the performance of our two-stage model.},
  issue = {9},
  langid = {english},
  keywords = {BI-RADS,breast cancer detection,breast cancer diagnosis,deep learning,optimization},
  file = {/home/caribu/Zotero/storage/CFS9CCIK/Ibrokhimov y Kang - 2022 - Two-Stage Deep Learning Method for Breast Cancer Detection Using High-Resolution Mammogram Images.pdf}
}

@article{jassemDelaysDiagnosisTreatment2014,
  title = {Delays in Diagnosis and Treatment of Breast Cancer: A Multinational Analysis},
  shorttitle = {Delays in Diagnosis and Treatment of Breast Cancer},
  author = {Jassem, Jacek and Ozmen, Vahit and Bacanu, Florin and Drobniene, Monika and Eglitis, Janis and Lakshmaiah, Kuntegowdanahalli C. and Kahan, Zsuzsanna and Mardiak, Jozef and Pieńkowski, Tadeusz and Semiglazova, Tatiana and Stamatovic, Ljiljana and Timcheva, Constanta and Vasovic, Suzana and Vrbanec, Damir and Zaborek, Piotr},
  date = {2014-10-01},
  journaltitle = {European Journal of Public Health},
  shortjournal = {European Journal of Public Health},
  volume = {24},
  number = {5},
  pages = {761--767},
  issn = {1101-1262},
  doi = {10.1093/eurpub/ckt131},
  url = {https://doi.org/10.1093/eurpub/ckt131},
  urldate = {2021-05-19},
  abstract = {Background: Reducing treatment delay improves outcomes in breast cancer. The aim of this study was to determine factors influencing patient- and system-related delays in commencing breast cancer treatment in different countries. Methods: A total of 6588 female breast cancer patients from 12 countries were surveyed. Total delay time was determined as the sum of the patient-related delay time (time between onset of the first symptoms and the first medical visit) and system-related delay time (time between the first medical visit and the start of therapy). Results: The average patient-related delay time and total delay time were 4.7 (range: 3.4–6.2) weeks and 14.4 (range: 11.5–29.4) weeks, respectively. Longer patient-related delay times were associated with distrust and disregard, and shorter patient-related delay times were associated with fear of breast cancer, practicing self-examination, higher education level, being employed, having support from friends and family and living in big cities. The average system-related delay time was 11.1 (range: 8.3–24.7) weeks. Cancer diagnosis made by an oncologist versus another physician, higher education level, older age, family history of female cancers and having a breast lump as the first cancer sign were associated with shorter system-related delay times. Longer patient-related delay times and higher levels of distrust and disregard were predictors of longer system-related delay times. Conclusions: The delay in diagnosis and treatment of breast cancer remains a serious problem. Several psychological and behavioural patient attributes strongly determine both patient-related delay time and system-related delay time, but their strength is different in particular countries.},
  file = {/home/caribu/Zotero/storage/AF55H7RQ/Jassem et al. - 2014 - Delays in diagnosis and treatment of breast cancer.pdf;/home/caribu/Zotero/storage/S8982N9K/474150.html}
}

@article{jiangLongitudinalAnalysisChange2023,
  title = {Longitudinal {{Analysis}} of {{Change}} in {{Mammographic Density}} in {{Each Breast}} and {{Its Association With Breast Cancer Risk}}},
  author = {Jiang, Shu and Bennett, Debbie L. and Rosner, Bernard A. and Colditz, Graham A.},
  date = {2023-06-01},
  journaltitle = {JAMA Oncology},
  shortjournal = {JAMA Oncol},
  volume = {9},
  number = {6},
  pages = {808},
  issn = {2374-2437},
  doi = {10.1001/jamaoncol.2023.0434},
  url = {https://jamanetwork.com/journals/jamaoncology/fullarticle/2804130},
  urldate = {2025-01-29},
  abstract = {OBJECTIVE To prospectively evaluate the association between change in mammographic density in each breast over time and risk of subsequent breast cancer. DESIGN, SETTING, AND PARTICIPANTS This nested case-control cohort study was sampled from the Joanne Knight Breast Health Cohort of 10 481 women free from cancer at entry and observed from November 3, 2008, to October 31, 2020, with routine screening mammograms every 1 to 2 years, providing a measure of breast density. Breast cancer screening was provided for a diverse population of women in the St Louis region. A total of 289 case patients with pathology-confirmed breast cancer were identified, and approximately 2 control participants were sampled for each case according to age at entry and year of enrollment, yielding 658 controls with a total number of 8710 craniocaudal-view mammograms for analysis. EXPOSURES Exposures included screening mammograms with volumetric percentage of density, change in volumetric breast density over time, and breast biopsy pathologyconfirmed cancer. Breast cancer risk factors were collected via questionnaire at enrollment. MAIN OUTCOMES AND MEASURES Longitudinal changes over time in each woman’s volumetric breast density by case and control status. RESULTS The mean (SD) age of the 947 participants was 56.67 (8.71) years at entry; 141 were Black (14.9\%), 763 were White (80.6\%), 20 were of other race or ethnicity (2.1\%), and 23 did not report this information (2.4\%). The mean (SD) interval was 2.0 (1.5) years from last mammogram to date of subsequent breast cancer diagnosis (10th percentile, 1.0 year; 90th percentile, 3.9 years). Breast density decreased over time in both cases and controls. However, there was a significantly slower decrease in rate of decline in density in the breast that developed breast cancer compared with the decline in controls (estimate = 0.027; 95\% CI, 0.001-0.053; P = .04). CONCLUSIONS AND RELEVANCE This study found that the rate of change in breast density was associated with the risk of subsequent breast cancer. Incorporation of longitudinal changes into existing models could optimize risk stratification and guide more personalized risk management.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/BHZ38W4P/Jiang et al. - 2023 - Longitudinal Analysis of Change in Mammographic Density in Each Breast and Its Association With Brea.pdf}
}

@article{JMLR:v12:pedregosa11a,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@article{kimBreastDensity2024,
  title = {Breast {{Density}}},
  author = {Kim, Eric and Lewin, Alana A.},
  date = {2024-07},
  journaltitle = {Radiologic Clinics of North America},
  shortjournal = {Radiologic Clinics of North America},
  volume = {62},
  number = {4},
  pages = {593--605},
  issn = {00338389},
  doi = {10.1016/j.rcl.2023.12.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0033838923002270},
  urldate = {2025-01-20},
  langid = {english},
  file = {/home/caribu/Zotero/storage/WN3KLV3V/Kim y Lewin - 2024 - Breast Density.pdf}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2024-02-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/K4S3MJ43/Kingma y Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/caribu/Zotero/storage/BYHUPJA6/1412.html}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2025-07-29},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/MUQ6AXW9/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf}
}

@book{lakhaniBreastTumours2019,
  title = {Breast Tumours},
  editor = {family=Lakhani, given=SR, given-i=SR and family=Ellis, given=IO, given-i=IO and family=Schnitt, given=SJ, given-i=SJ and family=Tan, given=PH, given-i=PH and family=Van de Vijver, given=MJ, given-i=MJ},
  date = {2019},
  series = {{{WHO Classification}} of {{Tumours}}},
  edition = {5th ed},
  volume = {2},
  publisher = {WHO Classification of Tumours Editorial Board},
  location = {Geneva},
  isbn = {978-92-832-4500-1},
  langid = {english}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2025-08-11},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  keywords = {Computer science,Mathematics and computing}
}

@article{liaoClassificationAsymmetryMammography2023,
  title = {Classification of Asymmetry in Mammography via the {{DenseNet}} Convolutional Neural Network},
  author = {Liao, Tingting and Li, Lin and Ouyang, Rushan and Lin, Xiaohui and Lai, Xiaohui and Cheng, Guanxun and Ma, Jie},
  date = {2023-12-01},
  journaltitle = {European Journal of Radiology Open},
  shortjournal = {European Journal of Radiology Open},
  volume = {11},
  pages = {100502},
  issn = {2352-0477},
  doi = {10.1016/j.ejro.2023.100502},
  url = {https://www.sciencedirect.com/science/article/pii/S235204772300028X},
  urldate = {2024-05-08},
  abstract = {Purpose To investigate the effectiveness of a deep learning system based on the DenseNet convolutional neural network in diagnosing benign and malignant asymmetric lesions in mammography. Methods Clinical and image data from 460 women aged 23–82 years (47.57~±~8.73 years) with asymmetric lesions who underwent mammography at Shenzhen People's Hospital, Shenzhen Luohu District People's Hospital, and Shenzhen Hospital of Peking University from December 2019 to December 2020 were retrospectively analyzed. Two senior radiologists, two junior radiologists, and the DL system read the mammographic images of 460 patients, respectively, and finally recorded the BI-RADS classification of asymmetric lesions. We then used the area under the curve (AUC) of the receiver operating characteristic (ROC) to evaluate the diagnostic efficacy and the difference between AUCs by the Delong method. Results Specificity (0.909 vs. 0.835, 0.790, χ2=8.21 and 17.22, p＜0.05) and precision (0.872 vs. 0.763, 0.726, χ2=9.23 and 5.22, p＜0.05) of the DL system in the diagnosis of benign and malignant asymmetric lesions were higher than those of junior radiologist A and B, and there was a statistically significant difference between AUCs (0.778 vs. 0.579, 0.564, Z\,=\,4.033 and 4.460, p＜0.05). Furthermore, the AUC (0.778 vs. 0.904, 0.862, Z\,=\,3.191, and 2.167, p＜0.05) of benign and malignant asymmetric lesions diagnosed by the DL system was lower than that of senior radiologist A and senior radiologist B. Conclusions The DL system based on the DenseNet convolution neural network has high diagnostic efficiency, which can help junior radiologists evaluate benign and malignant asymmetric lesions more accurately. It can also improve diagnostic accuracy and reduce missed diagnoses caused by inexperienced junior radiologists.},
  keywords = {Artificial Intelligence,Asymmetry,Deep learning,Mammography},
  file = {/home/caribu/Zotero/storage/KBIWIK2X/Liao et al. - 2023 - Classification of asymmetry in mammography via the.pdf;/home/caribu/Zotero/storage/FXU9LITH/S235204772300028X.html}
}

@online{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2018-07-13},
  eprint = {1807.05118},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.05118},
  url = {http://arxiv.org/abs/1807.05118},
  urldate = {2025-06-24},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  pubstate = {prepublished},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/WEPLNNMI/Liaw et al. - 2018 - Tune A Research Platform for Distributed Model Selection and Training.pdf;/home/caribu/Zotero/storage/5U8DJMJI/1807.html}
}

@online{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2023-07-13},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/H29ENDXW/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf;/home/caribu/Zotero/storage/9R438W5W/1708.html}
}

@article{liptonMythosModelInterpretability2018,
  title = {The {{Mythos}} of {{Model Interpretability}}: {{In}} Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  shorttitle = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  date = {2018-06-01},
  journaltitle = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  issn = {1542-7730},
  doi = {10.1145/3236386.3241340},
  url = {https://dl.acm.org/doi/10.1145/3236386.3241340},
  urldate = {2025-08-10},
  abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
  file = {/home/caribu/Zotero/storage/GVZWRF9A/Lipton - 2018 - The Mythos of Model Interpretability In machine learning, the concept of interpretability is both i.pdf}
}

@online{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  eprint = {2103.14030},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.14030},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2023-11-10},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/EPNSIYPW/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;/home/caribu/Zotero/storage/C3AK5Y7S/2103.html}
}

@article{liuWeaklysupervisedHighresolutionSegmentation2021,
  title = {Weakly-Supervised {{High-resolution Segmentation}} of {{Mammography Images}} for {{Breast Cancer Diagnosis}}},
  author = {Liu, Kangning and Shen, Yiqiu and Wu, Nan and Chłędowski, Jakub and Fernandez-Granda, Carlos and Geras, Krzysztof J.},
  date = {2021-07},
  journaltitle = {Proceedings of Machine Learning Research},
  shortjournal = {Proc Mach Learn Res},
  volume = {143},
  eprint = {35088055},
  eprinttype = {pubmed},
  pages = {268--285},
  issn = {2640-3498},
  abstract = {In the last few years, deep learning classifiers have shown promising results in image-based medical diagnosis. However, interpreting the outputs of these models remains a challenge. In cancer diagnosis, interpretability can be achieved by localizing the region of the input image responsible for the output, i.e. the location of a lesion. Alternatively, segmentation or detection models can be trained with pixel-wise annotations indicating the locations of malignant lesions. Unfortunately, acquiring such labels is labor-intensive and requires medical expertise. To overcome this difficulty, weakly-supervised localization can be utilized. These methods allow neural network classifiers to output saliency maps highlighting the regions of the input most relevant to the classification task (e.g. malignant lesions in mammograms) using only image-level labels (e.g. whether the patient has cancer or not) during training. When applied to high-resolution images, existing methods produce low-resolution saliency maps. This is problematic in applications in which suspicious lesions are small in relation to the image size. In this work, we introduce a novel neural network architecture to perform weakly-supervised segmentation of high-resolution images. The proposed model selects regions of interest via coarse-level localization, and then performs fine-grained segmentation of those regions. We apply this model to breast cancer diagnosis with screening mammography, and validate it on a large clinically-realistic dataset. Measured by Dice similarity score, our approach outperforms existing methods by a large margin in terms of localization performance of benign and malignant lesions, relatively improving the performance by 39.6\% and 20.0\%, respectively. Code and the weights of some of the models are available at https://github.com/nyukat/GLAM.},
  langid = {english},
  pmcid = {PMC8791642},
  keywords = {breast cancer screening,high-resolution medical images,weakly supervised learning}
}

@article{louMGBNConvolutionalNeural2021,
  title = {{{MGBN}}: {{Convolutional}} Neural Networks for Automated Benign and Malignant Breast Masses Classification},
  shorttitle = {{{MGBN}}},
  author = {Lou, Meng and Wang, Runze and Qi, Yunliang and Zhao, Wenwei and Xu, Chunbo and Meng, Jie and Deng, Xiangyu and Ma, Yide},
  date = {2021-07},
  journaltitle = {Multimedia Tools and Applications},
  shortjournal = {Multimed Tools Appl},
  volume = {80},
  number = {17},
  pages = {26731--26750},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-021-10929-6},
  url = {https://link.springer.com/10.1007/s11042-021-10929-6},
  urldate = {2024-10-10},
  abstract = {Automated benign and malignant breast masses classification is a crucial yet challenging topic. Recently, many studies based on convolutional neural network (CNN) are presented to address this task, but most of these CNN-based methods neglect the effective global contextual information. Moreover, their methods do not further analyze the reliability and interpretability of CNN models, which does not correspond to the clinical diagnosis. In this work, we firstly propose a novel multi-level global-guided branch-attention network (MGBN) for mass classification, which aims to fully leverage the multi-level global contextual information to refine the feature representation. Specifically, the MGBN includes a stem module and a branch module. The former extracts the local information through standard local convolutional operations of ResNet-50. The latter embeds the global contextual information and establishes the relationships of different feature levels via global pooling and Multi-layer Perceptron (MLP). The final prediction is computed by local information and global information together. Then, we discuss the reliability and interpretability of our mass classification network by visualizing the coarse localization map through Gradientweighted Class Activation Mapping (Grad-CAM), which is important in clinical diagnosis. Finally, our proposed MGBN is greatly demonstrated on two public mammographic mass classification databases including the DDSM and INbreast databases, resulting in AUC of 0.8375 and 0.9311, respectively.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/E9AYQVL6/Lou et al. - 2021 - MGBN Convolutional neural networks for automated benign and malignant breast masses classification.pdf}
}

@article{loyola-gonzalezBlackBoxVsWhiteBox2019,
  title = {Black-{{Box}} vs. {{White-Box}}: {{Understanding Their Advantages}} and {{Weaknesses From}} a {{Practical Point}} of {{View}}},
  shorttitle = {Black-{{Box}} vs. {{White-Box}}},
  author = {Loyola-González, Octavio},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {154096--154113},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2949286},
  url = {https://ieeexplore.ieee.org/document/8882211/?arnumber=8882211},
  urldate = {2024-07-26},
  abstract = {Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological neural networks,Biological system modeling,Black-box,Computational modeling,deep learning,explainable artificial intelligence,Gallium nitride,Machine learning,Mathematical model,Statistical analysis,white-box},
  file = {/home/caribu/Zotero/storage/5VNP9L2X/Loyola-González - 2019 - Black-Box vs. White-Box Understanding Their Advan.pdf;/home/caribu/Zotero/storage/CP3T7D3P/8882211.html}
}

@article{luDyingReLUInitialization2020,
  title = {Dying {{ReLU}} and {{Initialization}}: {{Theory}} and {{Numerical Examples}}},
  shorttitle = {Dying {{ReLU}} and {{Initialization}}},
  author = {Lu, Lu and Shin, Yeonjong and Su, Yanhui and Karniadakis, George Em},
  date = {2020-01-01},
  journaltitle = {Communications in Computational Physics},
  shortjournal = {CICP},
  volume = {28},
  number = {5},
  eprint = {1903.06733},
  eprinttype = {arXiv},
  eprintclass = {stat},
  pages = {1671--1706},
  issn = {1991-7120, 1815-2406},
  doi = {10.4208/cicp.OA-2020-0165},
  url = {http://arxiv.org/abs/1903.06733},
  urldate = {2025-07-29},
  abstract = {The dying ReLU refers to the problem when ReLU neurons become inactive and only output 0 for any input. There are many empirical and heuristic explanations of why ReLU neurons die. However, little is known about its theoretical analysis. In this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric probability distributions, which suffers from the dying ReLU. We thus propose a new initialization procedure, namely, a randomized asymmetric initialization. We show that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are provided to demonstrate the effectiveness of the new initialization procedure.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/CW2YLPZC/Lu et al. - 2020 - Dying ReLU and Initialization Theory and Numerical Examples.pdf}
}

@online{lundbergUnifiedApproachInterpreting2017a,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  date = {2017-11-25},
  eprint = {1705.07874},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.07874},
  url = {http://arxiv.org/abs/1705.07874},
  urldate = {2025-08-09},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/XUHV6KMW/Lundberg y Lee - 2017 - A Unified Approach to Interpreting Model Predictions.pdf;/home/caribu/Zotero/storage/C4BRH3JK/1705.html}
}

@article{maatenVisualizingDataUsing2008,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {family=Maaten, given=Laurens, prefix=van der, useprefix=false and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of machine learning research},
  volume = {9},
  pages = {2579--2605},
  url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
  urldate = {2025-08-10},
  issue = {Nov},
  file = {/home/caribu/Zotero/storage/IB3UPZLC/Maaten y Hinton - 2008 - Visualizing data using t-SNE.pdf}
}

@article{madariagaBreastCancerTrends2024,
  title = {Breast Cancer Trends in {{Chile}}: {{Incidence}} and Mortality Rates (2007–2018)},
  shorttitle = {Breast Cancer Trends in {{Chile}}},
  author = {Madariaga, Benjamín and Mondschein, Susana and Torres, Soledad},
  date = {2024-06-27},
  journaltitle = {PLOS Global Public Health},
  shortjournal = {PLOS Global Public Health},
  volume = {4},
  number = {6},
  pages = {e0001322},
  publisher = {Public Library of Science},
  issn = {2767-3375},
  doi = {10.1371/journal.pgph.0001322},
  url = {https://journals.plos.org/globalpublichealth/article?id=10.1371/journal.pgph.0001322},
  urldate = {2025-07-31},
  abstract = {Breast cancer (BC) is one of the most common cancers in women worldwide and in Chile. Due to the lack of a Chilean national cancer registry, there is partial information on the status of BC in the country. We aim to estimate BC incidence and mortality rates by health care providers and regions for Chilean women. We used two public anonymized databases provided by the Ministry of Health: the national death and hospital discharges datasets. We considered a cohort of 58,254 and 16,615 BC hospital discharges and deaths for the period 2007–2018. New BC cases increased by 43.6\%, from 3,785 in 2007 to 5,435 in 2018. Total BC deaths increased by 33.6\% from 1,158 to 1,547 during the same period. Age-adjusted incidence rates were stable over time, with an average rate of 44.0 cases/100,000 women (SD 2.2). There were considerable differences in age-adjusted incidence rates among regions, with no clear geographical trend. Women affiliated to a private provider (ISAPRE) have an average age-adjusted incidence rate of 60.6 compared to 38.8 (both cases/100,000 women) for women affiliated with the public provider (FONASA). Age-adjusted mortality rates have an average of 10.5 cases/100,000 women (SD 0.4). This study shows important differences in incidence rates between private and publicly insured women, with no significant differences in mortality rates. Such differences may be associated with women’s lifestyles, dietary compositions, comorbidities, and differences in healthcare systems. These hypotheses should be studied in greater depth. Additionally, differences in BC incidence found in this study compared to incidences reported from other estimations reinforce the need of a national cancer registry that should lead to more accurate indicators regarding BC in Chile.},
  langid = {english},
  keywords = {Breast cancer,Cancer detection and diagnosis,Cancer epidemiology,Cancer treatment,Chile (country),Death rates,Epidemiology,Health insurance},
  file = {/home/caribu/Zotero/storage/VD4GPNAR/Madariaga et al. - 2024 - Breast cancer trends in Chile Incidence and mortality rates (2007–2018).pdf}
}

@incollection{magnyBreastImagingReporting2025,
  title = {Breast {{Imaging Reporting}} and {{Data System}}},
  booktitle = {{{StatPearls}}},
  author = {Magny, Samuel J. and Shikhman, Rachel and Keppke, Ana L.},
  date = {2025},
  eprint = {29083600},
  eprinttype = {pubmed},
  publisher = {StatPearls Publishing},
  location = {Treasure Island (FL)},
  url = {http://www.ncbi.nlm.nih.gov/books/NBK459169/},
  urldate = {2025-08-11},
  abstract = {Breast imaging-reporting and data system (BI-RADS) is a classification system proposed by the American College of Radiology (ACR) in 1986 with the original report released in 1993. The 1980s saw an exponential increase in mammography with the implementation of yearly screening mammograms and overwhelming variation amongst radiology reports. BI-RADS~was implemented to standardize~risk assessment and quality control for mammography and provide uniformity in the reports for~non-radiologist. The first version proposed included the suggested structure for a mammographic report, the lexicon for mammographic imaging findings, and final assessment category with recommendations for management. The ACR used scientific analysis and literature review to create a lexicon of~descriptors that had shown to correlate with~high predictive values associated with either benign or malignant disease. The second important aspect of the BI-RADS system was the category classification for the overall assessment of the imaging findings. The categorization provides an approximate risk of malignancy to a lesion from essentially zero to greater than 95\%. The categorization and final assessment decreased ambiguity in recommendations. BI-RADS was built to be fluid and change with the adaptation of new techniques and research.~Such changes that have occurred are the~inclusion of~lexicons for ultrasound in 2003 and MRI in 2006. The latest edition is BI-RADS 5 (2013) and included six classifications for lesions.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/43HU6TRK/NBK459169.html}
}

@article{mcbeeDeepLearningRadiology2018,
  title = {Deep {{Learning}} in {{Radiology}}},
  author = {McBee, Morgan P. and Awan, Omer A. and Colucci, Andrew T. and Ghobadi, Comeron W. and Kadom, Nadja and Kansagra, Akash P. and Tridandapani, Srini and Auffermann, William F.},
  date = {2018-11},
  journaltitle = {Academic Radiology},
  shortjournal = {Academic Radiology},
  volume = {25},
  number = {11},
  pages = {1472--1480},
  issn = {10766332},
  doi = {10.1016/j.acra.2018.02.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1076633218301041},
  urldate = {2025-08-11},
  langid = {english},
  file = {/home/caribu/Zotero/storage/9VAXPR5S/McBee et al. - 2018 - Deep Learning in Radiology.pdf}
}

@online{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2020-09-18},
  eprint = {1802.03426},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1802.03426},
  url = {http://arxiv.org/abs/1802.03426},
  urldate = {2025-08-10},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/V24PEMKS/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf;/home/caribu/Zotero/storage/PIZ4PG8U/1802.html}
}

@inproceedings{melladoDeepLearningClassifier2023,
  title = {A {{Deep Learning Classifier Using Sliding Patches For Detection}} of {{Mammographical Findings}}},
  booktitle = {2023 19th {{International Symposium}} on {{Medical Information Processing}} and {{Analysis}} ({{SIPAIM}})},
  author = {Mellado, Diego and Querales, Marvin and Sotelo, Julio and Godoy, Eduardo and Pardo, Fabian and Lever, Scarlett and Chabert, Steren and Salas, Rodrigo},
  date = {2023-11},
  pages = {1--5},
  doi = {10.1109/SIPAIM56729.2023.10373511},
  url = {https://ieeexplore.ieee.org/abstract/document/10373511},
  urldate = {2024-01-08},
  abstract = {Mammography is known as one of the best forms to screen possible breast cancer in women, and recently deep learning models have been developed to assist the radiologist in the diagnosis. However, their lack of interpretability has become a significant drawback to their extended use in clinical practice. This paper introduces a novel approach for detecting and localising pathological findings in mammography exams through the use of a EfficientNet-based deep learning model. The model is trained using cropped segments of labelled pathological findings from Vindr Mammography Dataset. Achieving an average F1-score of 72.7 \%, and reaching on mass and suspicious calcifications an F1-Score of 79.9 \% and 84.5 \% respectively. Using this classifier we propose a method to visualise from local information the regions of interest where pathological findings could be present on the complete image. Plus, we describe the limitations regarding area coverage of these patches on the model’s capability of generalization and certainty on its predictions, explaining its functionality.},
  eventtitle = {2023 19th {{International Symposium}} on {{Medical Information Processing}} and {{Analysis}} ({{SIPAIM}})},
  file = {/home/caribu/Zotero/storage/C7SVGJUA/Mellado et al. - 2023 - A Deep Learning Classifier Using Sliding Patches For Detection of Mammographical Findings.pdf}
}

@article{millerExplanationArtificialIntelligence2019,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  shorttitle = {Explanation in Artificial Intelligence},
  author = {Miller, Tim},
  date = {2019-02-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2018.07.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
  urldate = {2025-01-27},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency},
  file = {/home/caribu/Zotero/storage/35MDMEYY/Miller - 2019 - Explanation in artificial intelligence Insights from the social sciences.pdf;/home/caribu/Zotero/storage/YDEP2L5G/S0004370218305988.html}
}

@report{ministeriodesaludInformeVigilanciaCancer2021,
  title = {Informe de {{Vigilancia}} de {{Cáncer}}. {{Análisis}} de {{Mortalidad Prematura}} y {{AVPP}} Por {{Cáncer}}. {{Década}} 2009-2018.},
  author = {{Ministerio de Salud}},
  date = {2021},
  institution = {Departamento de Epidemiologia},
  location = {Chile},
  url = {https://www.minsal.cl/wp-content/uploads/2022/01/Informe-Mortalidad-Prematura-y-AVPP-por-C%C3%A1ncer-2009-2018.pdf},
  urldate = {2023-11-09},
  file = {/home/caribu/Zotero/storage/45U8R6N7/Informe-Mortalidad-Prematura-y-AVPP-por-Cáncer-2009-2018.pdf}
}

@book{molnarInterpretableMachineLearning2025,
  title = {Interpretable Machine Learning: {{A}} Guide for Making Black Box Models Explainable},
  shorttitle = {Interpretable Machine Learning},
  author = {Molnar, Christoph},
  date = {2025},
  edition = {3:e upplagan},
  publisher = {Christoph Molnar},
  location = {München},
  isbn = {978-3-911578-03-5},
  langid = {english},
  annotation = {OCLC: 1520125425}
}

@article{momenimovahedEpidemiologicalCharacteristicsRisk2019,
  title = {Epidemiological Characteristics of and Risk Factors for Breast Cancer in the World},
  author = {Momenimovahed, Zohre and Salehiniya, Hamid},
  date = {2019-04-10},
  journaltitle = {Breast Cancer : Targets and Therapy},
  shortjournal = {Breast Cancer (Dove Med Press)},
  volume = {11},
  eprint = {31040712},
  eprinttype = {pubmed},
  pages = {151--164},
  issn = {1179-1314},
  doi = {10.2147/BCTT.S176070},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6462164/},
  urldate = {2021-07-11},
  abstract = {Aim Breast cancer is the most common cancer among women and one of the most important causes of death among them. This review aimed to investigate the incidence and mortality rates of breast cancer and to identify the risk factors for breast cancer in the world. Materials and methods A search was performed in PubMed, Web of Science, and Scopus databases without any time restrictions. The search keywords included the following terms: breast cancer, risk factors, incidence, and mortality and a combination of these terms. Studies published in English that referred to various aspects of breast cancer including epidemiology and risk factors were included in the study. Overall, 142 articles published in English were included in the study. Results Based on the published studies, the incidence rate of breast cancer varies greatly with race and ethnicity and is higher in developed countries. Results of this study show that mortality rate of breast cancer is higher in less developed regions. The findings of this study demonstrated that various risk factors including demographic, reproductive, hormonal, hereditary, breast related, and lifestyle contribute to the incidence of breast cancer. Conclusion The results of this study indicated that incidence and mortality rates of breast cancer is rising, so design and implementation of screening programs and the control of risk factors seem essential.},
  pmcid = {PMC6462164},
  file = {/home/caribu/Zotero/storage/WQJS2KUZ/Momenimovahed y Salehiniya - 2019 - Epidemiological characteristics of and risk factors for breast cancer in the world.pdf}
}

@article{monticcioloBreastCancerScreening2023,
  title = {Breast {{Cancer Screening}} for {{Women}} at {{Higher-Than-Average Risk}}: {{Updated Recommendations From}} the {{ACR}}},
  shorttitle = {Breast {{Cancer Screening}} for {{Women}} at {{Higher-Than-Average Risk}}},
  author = {Monticciolo, Debra L. and Newell, Mary S. and Moy, Linda and Lee, Cindy S. and Destounis, Stamatia V.},
  date = {2023-09-01},
  journaltitle = {Journal of the American College of Radiology},
  shortjournal = {Journal of the American College of Radiology},
  volume = {20},
  number = {9},
  eprint = {37150275},
  eprinttype = {pubmed},
  pages = {902--914},
  publisher = {Elsevier},
  issn = {1546-1440, 1558-349X},
  doi = {10.1016/j.jacr.2023.04.002},
  url = {https://www.jacr.org/article/S1546-1440(23)00334-4/fulltext},
  urldate = {2024-07-18},
  langid = {english},
  keywords = {breast cancer,breast cancer risk assessment,Breast cancer screening,breast MRI,higher risk populations},
  file = {/home/caribu/Zotero/storage/YW9DYFL9/Monticciolo et al. - 2023 - Breast Cancer Screening for Women at Higher-Than-A.pdf}
}

@article{moreiraINbreastFullfieldDigital2012,
  title = {{{INbreast}}: Toward a Full-Field Digital Mammographic Database},
  shorttitle = {{{INbreast}}},
  author = {Moreira, Inês C. and Amaral, Igor and Domingues, Inês and Cardoso, António and Cardoso, Maria João and Cardoso, Jaime S.},
  date = {2012-02},
  journaltitle = {Academic Radiology},
  shortjournal = {Acad Radiol},
  volume = {19},
  number = {2},
  eprint = {22078258},
  eprinttype = {pubmed},
  pages = {236--248},
  issn = {1878-4046},
  doi = {10.1016/j.acra.2011.09.014},
  abstract = {RATIONALE AND OBJECTIVES: Computer-aided detection and diagnosis (CAD) systems have been developed in the past two decades to assist radiologists in the detection and diagnosis of lesions seen on breast imaging exams, thus providing a second opinion. Mammographic databases play an important role in the development of algorithms aiming at the detection and diagnosis of mammary lesions. However, available databases often do not take into consideration all the requirements needed for research and study purposes. This article aims to present and detail a new mammographic database. MATERIALS AND METHODS: Images were acquired at a breast center located in a university hospital (Centro Hospitalar de S. João [CHSJ], Breast Centre, Porto) with the permission of the Portuguese National Committee of Data Protection and Hospital's Ethics Committee. MammoNovation Siemens full-field digital mammography, with a solid-state detector of amorphous selenium was used. RESULTS: The new database-INbreast-has a total of 115 cases (410 images) from which 90 cases are from women with both breasts affected (four images per case) and 25 cases are from mastectomy patients (two images per case). Several types of lesions (masses, calcifications, asymmetries, and distortions) were included. Accurate contours made by specialists are also provided in XML format. CONCLUSION: The strengths of the actually presented database-INbreast-relies on the fact that it was built with full-field digital mammograms (in opposition to digitized mammograms), it presents a wide variability of cases, and is made publicly available together with precise annotations. We believe that this database can be a reference for future works centered or related to breast cancer imaging.},
  langid = {english},
  keywords = {Breast Neoplasms,Databases Factual,Female,Humans,Mammography,Portugal,Radiology Information Systems},
  file = {/home/caribu/Zotero/storage/JAY2JUDE/Moreira et al. - 2012 - INbreast toward a full-field digital mammographic database.pdf}
}

@online{nguyenNovelMultiviewDeep2022,
  title = {A Novel Multi-View Deep Learning Approach for {{BI-RADS}} and Density Assessment of Mammograms},
  author = {Nguyen, Huyen T. X. and Tran, Sam B. and Nguyen, Dung B. and Pham, Hieu H. and Nguyen, Ha Q.},
  date = {2022-04-17},
  eprint = {2112.04490},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2112.04490},
  url = {http://arxiv.org/abs/2112.04490},
  urldate = {2024-12-17},
  abstract = {Advanced deep learning (DL) algorithms may predict the patient’s risk of developing breast cancer based on the Breast Imaging Reporting and Data System (BI-RADS) and density standards. Recent studies have suggested that the combination of multi-view analysis improved the overall breast exam classification. In this paper, we propose a novel multi-view DL approach for BI-RADS and density assessment of mammograms. The proposed approach first deploys deep convolutional networks for feature extraction on each view separately. The extracted features are then stacked and fed into a Light Gradient Boosting Machine (LightGBM) classifier to predict BI-RADS and density scores. We conduct extensive experiments on both the internal mammography dataset and the public dataset Digital Database for Screening Mammography (DDSM). The experimental results demonstrate that the proposed approach outperforms the single-view classification approach on two benchmark datasets by huge F1-score margins (+5\% on the internal dataset and +10\% on the DDSM dataset). These results highlight the vital role of combining multi-view information to improve the performance of breast cancer risk prediction.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/caribu/Zotero/storage/EZWR2HZY/Nguyen et al. - 2022 - A novel multi-view deep learning approach for BI-RADS and density assessment of mammograms.pdf}
}

@online{nguyenVinDrMammoLargescaleBenchmark2022,
  title = {{{VinDr-Mammo}}: {{A}} Large-Scale Benchmark Dataset for Computer-Aided Diagnosis in Full-Field Digital Mammography},
  shorttitle = {{{VinDr-Mammo}}},
  author = {Nguyen, Hieu T. and Nguyen, Ha Q. and Pham, Hieu H. and Lam, Khanh and Le, Linh T. and Dao, Minh and Vu, Van},
  date = {2022-03-10},
  eprinttype = {medRxiv},
  pages = {2022.03.07.22272009},
  doi = {10.1101/2022.03.07.22272009},
  url = {https://www.medrxiv.org/content/10.1101/2022.03.07.22272009v1},
  urldate = {2022-12-17},
  abstract = {Mammography, or breast X-ray, is the most widely used imaging modality to detect cancer and other breast diseases. Recent studies have shown that deep learning-based computer-assisted detection and diagnosis (CADe/x) tools have been developed to support physicians and improve the accuracy of interpreting mammography. However, most published datasets of mammography are either limited on sample size or digitalized from screen-film mammography (SFM), hindering the development of CADe/x tools which are developed based on full-field digital mammography (FFDM). To overcome this challenge, we introduce VinDr-Mammo – a new benchmark dataset of FFDM for detecting and diagnosing breast cancer and other diseases in mammography. The dataset consists of 5,000 mammography exams, each of which has four standard views and is double read with disagreement (if any) being resolved by arbitration. It is created for the assessment of Breast Imaging Reporting and Data System (BI-RADS) and density at the breast level. In addition, the dataset also provides the category, location, and BI-RADS assessment of non-benign findings. We make VinDr-Mammo publicly available on https://physionet.org/ as a new imaging resource to promote advances in developing CADe/x tools for breast cancer screening.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/caribu/Zotero/storage/4MCT3QAS/Nguyen et al. - 2022 - VinDr-Mammo A large-scale benchmark dataset for c.pdf}
}

@article{obeaguBreastCancerReview2024,
  title = {Breast Cancer: {{A}} Review of Risk Factors and Diagnosis},
  shorttitle = {Breast Cancer},
  author = {Obeagu, Emmanuel Ifeanyi and Obeagu, Getrude Uzoma},
  date = {2024-01-19},
  journaltitle = {Medicine},
  volume = {103},
  number = {3},
  pages = {e36905},
  issn = {0025-7974, 1536-5964},
  doi = {10.1097/MD.0000000000036905},
  url = {https://journals.lww.com/10.1097/MD.0000000000036905},
  urldate = {2025-08-05},
  abstract = {Breast cancer remains a complex and prevalent health concern affecting millions of individuals worldwide. This review paper presents a comprehensive analysis of the multifaceted landscape of breast cancer, elucidating the diverse spectrum of risk factors contributing to its occurrence and exploring advancements in diagnostic methodologies. Through an extensive examination of current literature, various risk factors have been identified, encompassing genetic predispositions such as BRCA mutations, hormonal influences, lifestyle factors, and reproductive patterns. Age, family history, and environmental factors further contribute to the intricate tapestry of breast cancer etiology. Moreover, this review delineates the pivotal role of diagnostic tools in the early detection and management of breast cancer. Mammography, the cornerstone of breast cancer screening, is augmented by emerging technologies like magnetic resonance imaging and molecular testing, enabling improved sensitivity and specificity in diagnosing breast malignancies. Despite these advancements, challenges persist in ensuring widespread accessibility to screening programs, particularly in resource-limited settings. In conclusion, this review underscores the importance of understanding diverse risk factors in the development of breast cancer and emphasizes the critical role of evolving diagnostic modalities in enhancing early detection. The synthesis of current knowledge in this review aims to contribute to a deeper comprehension of breast cancer’s multifactorial nature and inform future directions in research, screening strategies, and preventive interventions.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/QQYILBHE/Obeagu y Obeagu - 2024 - Breast cancer A review of risk factors and diagnosis.pdf}
}

@article{opencv_library,
  title = {The {{OpenCV}} Library},
  author = {Bradski, G.},
  date = {2000},
  journaltitle = {Dr. Dobb's Journal of Software Tools},
  citeulike-article-id = {2236121},
  posted-at = {2008-01-15 19:21:54},
  priority = {4},
  keywords = {bibtex-import}
}

@article{otsuThresholdSelectionMethod1979,
  title = {A {{Threshold Selection Method}} from {{Gray-Level Histograms}}},
  author = {Otsu, Nobuyuki},
  date = {1979-01},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {9},
  number = {1},
  pages = {62--66},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1979.4310076},
  url = {https://ieeexplore.ieee.org/document/4310076},
  urldate = {2024-11-18},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}},
  keywords = {Displays,Gaussian distribution,Histograms,Least squares approximation,Marine vehicles,Q measurement,Radar tracking,Sea measurements,Surveillance,Target tracking},
  file = {/home/caribu/Zotero/storage/7RAU75D5/Otsu - 1979 - A Threshold Selection Method from Gray-Level Histograms.pdf;/home/caribu/Zotero/storage/CHPRZSWV/4310076.html}
}

@article{pertuzSaliencyBreastLesions2023,
  title = {Saliency of Breast Lesions in Breast Cancer Detection Using Artificial Intelligence},
  author = {Pertuz, Said and Ortega, David and Suarez, Érika and Cancino, William and Africano, Gerson and Rinta-Kiikka, Irina and Arponen, Otso and Paris, Sara and Lozano, Alfonso},
  date = {2023-11-23},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {13},
  number = {1},
  pages = {20545},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-46921-3},
  url = {https://www.nature.com/articles/s41598-023-46921-3},
  urldate = {2024-05-15},
  abstract = {Abstract             The analysis of mammograms using artificial intelligence (AI) has shown great potential for assisting breast cancer screening. We use saliency maps to study the role of breast lesions in the decision-making process of AI systems for breast cancer detection in screening mammograms. We retrospectively collected mammograms from 191 women with screen-detected breast cancer and 191 healthy controls matched by age and mammographic system. Two radiologists manually segmented the breast lesions in the mammograms from CC and MLO views. We estimated the detection performance of four deep learning-based AI systems using the area under the ROC curve (AUC) with a 95\% confidence interval (CI). We used automatic thresholding on saliency maps from the AI systems to identify the areas of interest on the mammograms. Finally, we measured the overlap between these areas of interest and the segmented breast lesions using Dice’s similarity coefficient (DSC). The detection performance of the AI systems ranged from low to moderate (AUCs from 0.525 to 0.694). The overlap between the areas of interest and the breast lesions was low for all the studied methods (median DSC from 4.2\% to 38.0\%). The AI system with the highest cancer detection performance (AUC\,=\,0.694, CI 0.662–0.726) showed the lowest overlap (DSC\,=\,4.2\%) with breast lesions. The areas of interest found by saliency analysis of the AI systems showed poor overlap with breast lesions. These results suggest that AI systems with the highest performance do not solely rely on localized breast lesions for their decision-making in cancer detection; rather, they incorporate information from large image regions. This work contributes to the understanding of the role of breast lesions in cancer detection using AI.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/K6GF8EBZ/Pertuz et al. - 2023 - Saliency of breast lesions in breast cancer detect.pdf}
}

@online{petsiukRISERandomizedInput2018,
  title = {{{RISE}}: {{Randomized Input Sampling}} for {{Explanation}} of {{Black-box Models}}},
  shorttitle = {{{RISE}}},
  author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
  date = {2018-09-25},
  eprint = {1806.07421},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.07421},
  url = {http://arxiv.org/abs/1806.07421},
  urldate = {2024-02-28},
  abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches. Project page: http://cs-people.bu.edu/vpetsiuk/rise/},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/E85MEW87/Petsiuk et al. - 2018 - RISE Randomized Input Sampling for Explanation of.pdf;/home/caribu/Zotero/storage/VULRN9NC/1806.html}
}

@article{pintoBarriersLatinAmerica2019,
  title = {Barriers in {{Latin America}} for the Management of Locally Advanced Breast Cancer},
  author = {Pinto, Joseph A and Pinillos, Luis and Villarreal-Garza, Cynthia and Morante, Zaida and Villarán, Manuel V and Mejía, Gerson and Caglevic, Christian and Aguilar, Alfredo and Fajardo, Williams and Usuga, Franz and Carrasco, Marcia and Rebaza, Pamela and Posada, Ana M and Tirado-Hurtado, Indira and Flores, Claudio and Vallejos, Carlos S},
  date = {2019-01-22},
  journaltitle = {ecancermedicalscience},
  shortjournal = {Ecancermedicalscience},
  volume = {13},
  eprint = {30792814},
  eprinttype = {pubmed},
  pages = {897},
  issn = {1754-6605},
  doi = {10.3332/ecancer.2019.897},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6372299/},
  urldate = {2024-09-27},
  abstract = {Breast cancer (BC) is a highly prevalent malignancy in Latin American women, most cases being diagnosed at locally advanced or metastatic stages when options for cancer care are limited. Despite its label as a public health problem in the region, Latin American BC patients face several barriers in accessing standard of care treatment when compared with patients from developed countries. In this review, we analyse the landscape of the four main identified barriers in the region: i) high burden of locally advanced/advanced BC; ii) inadequate access to medical resources; iii) deficient access to specialised cancer care and iv) insufficient BC research in Latin America. Unfortunately, these barriers represent the main factors associated with the BC poor outcomes seen in the region. Targeted actions should be conducted independently by each country and as a region to overcome these limitations and create an enhanced model of BC care.},
  pmcid = {PMC6372299},
  file = {/home/caribu/Zotero/storage/SE4HCJLZ/Pinto et al. - 2019 - Barriers in Latin America for the management of locally advanced breast cancer.pdf}
}

@article{prinziRad4XCNNNewAgnostic2025,
  title = {{{Rad4XCNN}}: {{A}} New Agnostic Method for {\mkbibemph{Post-Hoc}} Global Explanation of {{CNN-derived}} Features by Means of {{Radiomics}}},
  shorttitle = {{{Rad4XCNN}}},
  author = {Prinzi, Francesco and Militello, Carmelo and Zarcaro, Calogero and Bartolotta, Tommaso Vincenzo and Gaglio, Salvatore and Vitabile, Salvatore},
  date = {2025-03-01},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  shortjournal = {Computer Methods and Programs in Biomedicine},
  volume = {260},
  pages = {108576},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2024.108576},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260724005698},
  urldate = {2025-07-17},
  abstract = {Background and Objective: In recent years, machine learning-based clinical decision support systems (CDSS) have played a key role in the analysis of several medical conditions. Despite their promising capabilities, the lack of transparency in AI models poses significant challenges, particularly in medical contexts where reliability is a mandatory aspect. However, it appears that explainability is inversely proportional to accuracy. For this reason, achieving transparency without compromising predictive accuracy remains a key challenge. Methods: This paper presents a novel method, namely Rad4XCNN, to enhance the predictive power of CNN-derived features with the inherent interpretability of radiomic features. Rad4XCNN diverges from conventional methods based on saliency maps, by associating intelligible meaning to CNN-derived features by means of Radiomics, offering new perspectives on explanation methods beyond visualization maps. Results: Using a breast cancer classification task as a case study, we evaluated Rad4XCNN on ultrasound imaging datasets, including an online dataset and two in-house datasets for internal and external validation. Some key results are: (i) CNN-derived features guarantee more robust accuracy when compared against ViT-derived and radiomic features; (ii) conventional visualization map methods for explanation present several pitfalls; (iii) Rad4XCNN does not sacrifice model accuracy for their explainability; (iv) Rad4XCNN provides a global explanation enabling the physician to extract global insights and findings. Conclusions: Our method can mitigate some concerns related to the explainability-accuracy trade-off. This study highlighted the importance of proposing new methods for model explanation without affecting their accuracy.},
  keywords = {Breast cancer,Clinical Decision Support Systems,Convolutional neural networks,Explainable AI,Radiomics},
  file = {/home/caribu/Zotero/storage/EXDKTM9H/Prinzi et al. - 2025 - Rad4XCNN A new agnostic method for post-hoc global explanation of CNN-derived features by me.pdf;/home/caribu/Zotero/storage/9BEII2UU/S0169260724005698.html}
}

@unpublished{quinnTrustMedicalAI2020,
  title = {Trust and {{Medical AI}}: {{The}} Challenges We Face and the Expertise Needed to Overcome Them},
  shorttitle = {Trust and {{Medical AI}}},
  author = {Quinn, Thomas P. and Senadeera, Manisha and Jacobs, Stephan and Coghlan, Simon and Le, Vuong},
  date = {2020-08-18},
  eprint = {2008.07734},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2008.07734},
  urldate = {2021-06-16},
  abstract = {Artificial intelligence (AI) is increasingly of tremendous interest in the medical field. However, failures of medical AI could have serious consequences for both clinical outcomes and the patient experience. These consequences could erode public trust in AI, which could in turn undermine trust in our healthcare institutions. This article makes two contributions. First, it describes the major conceptual, technical, and humanistic challenges in medical AI. Second, it proposes a solution that hinges on the education and accreditation of new expert groups who specialize in the development, verification, and operation of medical AI technologies. These groups will be required to maintain trust in our healthcare institutions.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/home/caribu/Zotero/storage/AS534I9X/Quinn et al. - 2020 - Trust and Medical AI The challenges we face and t.pdf;/home/caribu/Zotero/storage/VH7CCACC/2008.html}
}

@inproceedings{raffertyExplainableArtificialIntelligence2022,
  title = {Explainable {{Artificial Intelligence}} for~{{Breast Tumour Classification}}: {{Helpful}} or~{{Harmful}}},
  shorttitle = {Explainable {{Artificial Intelligence}} for~{{Breast Tumour Classification}}},
  booktitle = {Interpretability of {{Machine Intelligence}} in {{Medical Image Computing}}},
  author = {Rafferty, Amy and Nenutil, Rudolf and Rajan, Ajitha},
  editor = {Reyes, Mauricio and Henriques Abreu, Pedro and Cardoso, Jaime},
  date = {2022},
  pages = {104--123},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-17976-1_10},
  abstract = {Explainable Artificial Intelligence (XAI) is the field of AI dedicated to promoting trust in machine learning models by helping us to understand how they make their decisions. For example, image explanations show us which pixels or segments were deemed most important by a model for a particular classification decision. This research focuses on image explanations generated by LIME, RISE and SHAP for a model which classifies breast mammograms as either benign or malignant. We assess these XAI techniques based on (1) the extent to which they agree with each other, as decided by One-Way ANOVA, Kendall’s Tau and RBO statistical tests, and (2) their agreement with the diagnostically important areas as identified by a radiologist on a small subset of mammograms. The main contribution of this research is the discovery that the 3 techniques consistently disagree both with each other and with the medical truth. We argue that using these off-shelf techniques in a medical context is not a feasible approach, and discuss possible causes of this problem, as well as some potential solutions.},
  isbn = {978-3-031-17976-1},
  langid = {english},
  file = {/home/caribu/Zotero/storage/CSQN8CMH/Rafferty et al. - 2022 - Explainable Artificial Intelligence for Breast Tum.pdf}
}

@article{rahmanBreastCancerDetection2024,
  title = {Breast {{Cancer Detection}} and {{Localizing}} the {{Mass Area Using Deep Learning}}},
  author = {Rahman, Md Mijanur and Jahangir, Md Zihad Bin and Rahman, Anisur and Akter, Moni and Nasim, MD Abdullah Al and Gupta, Kishor Datta and George, Roy},
  date = {2024-07},
  journaltitle = {Big Data and Cognitive Computing},
  volume = {8},
  number = {7},
  pages = {80},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-2289},
  doi = {10.3390/bdcc8070080},
  url = {https://www.mdpi.com/2504-2289/8/7/80},
  urldate = {2025-08-06},
  abstract = {Breast cancer presents a substantial health obstacle since it is the most widespread invasive cancer and the second most common cause of death in women. Prompt identification is essential for effective intervention, rendering breast cancer screening a critical component of healthcare. Although mammography is frequently employed for screening purposes, the manual diagnosis performed by pathologists can be laborious and susceptible to mistakes. Regrettably, the majority of research prioritizes mass classification over mass localization, resulting in an uneven distribution of attention. In response to this problem, we suggest a groundbreaking approach that seeks to identify and pinpoint cancers in breast mammography pictures. This will allow medical experts to identify tumors more quickly and with greater precision. This paper presents a complex deep convolutional neural network design that incorporates advanced deep learning techniques such as U-Net and YOLO. The objective is to enable automatic detection and localization of breast lesions in mammography pictures. To assess the effectiveness of our model, we carried out a thorough review that included a range of performance criteria. We specifically evaluated the accuracy, precision, recall, F1-score, ROC curve, and R-squared error using the publicly available MIAS dataset. Our model performed exceptionally well, with an accuracy rate of 93.0\% and an AUC (area under the curve) of 98.6\% for the detection job. Moreover, for the localization task, our model achieved a remarkably high R-squared value of 97\%. These findings highlight that deep learning can boost the efficiency and accuracy of diagnosing breast cancer. The automation of breast lesion detection and classification offered by our proposed method bears substantial benefits. By alleviating the workload burden on pathologists, it facilitates expedited and accurate breast cancer screening processes. As a result, the proposed approach holds promise for improving healthcare outcomes and bolstering the overall effectiveness of breast cancer detection and diagnosis.},
  issue = {7},
  langid = {english},
  keywords = {breast cancer detection,breast cancer localization,convolution neural network,deep learning,machine learning,MIAS mammography dataset},
  file = {/home/caribu/Zotero/storage/XXP24RN6/Rahman et al. - 2024 - Breast Cancer Detection and Localizing the Mass Area Using Deep Learning.pdf}
}

@article{raiExplainableAIBlack2020,
  title = {Explainable {{AI}}: From Black Box to Glass Box},
  shorttitle = {Explainable {{AI}}},
  author = {Rai, Arun},
  date = {2020-01-01},
  journaltitle = {Journal of the Academy of Marketing Science},
  shortjournal = {J. of the Acad. Mark. Sci.},
  volume = {48},
  number = {1},
  pages = {137--141},
  issn = {1552-7824},
  doi = {10.1007/s11747-019-00710-5},
  url = {https://doi.org/10.1007/s11747-019-00710-5},
  urldate = {2021-05-19},
  langid = {english},
  file = {/home/caribu/Zotero/storage/QJH34KLG/Rai - 2020 - Explainable AI from black box to glass box.pdf}
}

@article{rajalakshmiDeeplySupervisedUNet2020,
  title = {Deeply Supervised {{U}}‐{{Net}} for Mass Segmentation in Digital Mammograms},
  author = {Rajalakshmi, N Ravitha and Vidhyapriya, R and Elango, N and Ramesh, Nikhil},
  date = {2020},
  journaltitle = {International Journal of Imaging Systems and Technology},
  volume = {31},
  number = {1},
  pages = {59--71},
  doi = {10.1002/ima.22516},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/ima.22516},
  urldate = {2021-08-06},
  abstract = {Mass detection is a critical process in the examination of mammograms. The shape and texture of the mass are key parameters used in the diagnosis of breast cancer. To recover the shape of the mass, semantic segmentation is found to be more useful rather than mere object detection (or) localization. The main challenges involved in the mass segmentation include: (a) low signal to noise ratio (b) indiscernible mass boundaries, and (c) more false positives. These problems arise due to the significant overlap in the intensities of both the normal parenchymal region and the mass region. To address these challenges, deeply supervised U-Net model (DS U-Net) coupled with dense conditional random fields (CRFs) is proposed. Here, the input images are preprocessed using CLAHE and a modified encoder-decoder-based deep learning model is used for segmentation. In general, the encoder captures the textual information of various regions in an input image, whereas the decoder recovers the spatial location of the desired region of interest. The encoder-decoder-based models lack the ability to recover the non-conspicuous and spiculated mass boundaries. In the proposed work, deep supervision is integrated with a popular encoder-decoder model (U-Net) to improve the attention of the network toward the boundary of the suspicious regions. The final segmentation map is also created as a linear combination of the intermediate feature maps and the output feature map. The dense CRF is then used to fine-tune the segmentation map for the recovery of definite edges. The DS U-Net with dense CRF is evaluated on two publicly available benchmark datasets CBIS-DDSM and INBREAST. It provides a dice score of 82.9\% for CBIS-DDSM and 79\% for INBREAST.},
  file = {/home/caribu/Zotero/storage/Q26QPIA8/Rajalakshmi et al. - 2020 - Deeply supervised U‐Net for mass segmentation in d.pdf;/home/caribu/Zotero/storage/JAVHWM5Z/ima.html}
}

@online{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2016-01-06},
  eprint = {1506.01497},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.01497},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2025-05-11},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/GHI73WXB/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf;/home/caribu/Zotero/storage/R8PISQ5X/1506.html}
}

@article{renGlobalGuidelinesBreast2022,
  title = {Global Guidelines for Breast Cancer Screening: {{A}} Systematic Review},
  shorttitle = {Global Guidelines for Breast Cancer Screening},
  author = {Ren, Wenhui and Chen, Mingyang and Qiao, Youlin and Zhao, Fanghui},
  date = {2022-08-01},
  journaltitle = {The Breast},
  shortjournal = {The Breast},
  volume = {64},
  pages = {85--99},
  issn = {0960-9776},
  doi = {10.1016/j.breast.2022.04.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0960977622000765},
  urldate = {2025-07-31},
  abstract = {Objectives Breast cancer screening guidelines could provide valuable tools for clinical decision making by reviewing the available evidence and providing recommendations. Little information is known about how many countries have issued breast cancer screening guidelines and the differences among existing guidelines. We systematically reviewed current guidelines and summarized corresponding recommendations, to provide references for good clinical practice in different countries. Methods Systematic searches of MEDLINE, EMBASE, Web of Science, and Scopus from inception to March 27th, 2021 were conducted and supplemented by reviewing the guideline development organizations. The quality of screening guidelines was assessed from six domains of the Appraisal of Guidelines for Research and Evaluation Ⅱ (AGREE Ⅱ)~instrument by two appraisers. The basic information and recommendations of the issued guidelines were extracted and summarized. Results A total of 23 guidelines issued between 2010 and 2021 in 11 countries or regions were identified for further review. The content and quality varied across the guidelines. The average AGREE Ⅱ scores of the guidelines ranged from 33.3\% to 87.5\%. The highest domain score was "clarity of presentation" while the domain with the lowest score was "applicability". For average-risk women, most of the guidelines recommended mammographic screening for those aged 40–74 years, specifically, those aged 50–69 years were regarded as the optimal age group for screening. Nine of 23 guidelines recommended against an upper age limit for breast cancer screening. Mammography (MAM) was recommended as the primary screening modality for average-risk women by all included guidelines. Most guidelines suggested annual or biennial mammographic screening. Risk factors of breast cancer identified in the guidelines mainly fell within five categories which could be broadly summarized as the personal history of pre-cancerous lesions and/or breast cancer; the family history of breast cancer; the known genetic predisposition of breast cancer; the history of mantle or chest radiation therapy; and dense breasts. For women at higher risk, there was a consensus among most guidelines that annual MAM or annual magnetic resonance imaging (MRI) should be given, and the screening should begin earlier than the average-risk group. Conclusions The majority of 23 included international guidelines were issued by developed countries which contained roughly the same but not identical recommendations on breast cancer screening age, methods, and intervals. Most guidelines recommended annual or biennial mammographic screening between 40 and 74 years for average-risk populations and annual MAM or annual MRI starting from a younger age for high-risk populations. Current guidelines varied in quality and increased efforts are needed to improve the methodological~quality of guidance documents. Due to lacking clinical practice guidelines tailored to different economic levels, low- and middle-income countries (LMICs) should apply and implement the evidence-based guidelines with higher AGREE Ⅱ scores considering local adaption.},
  keywords = {Breast neoplasms,Guideline,Screening,Systematic review},
  file = {/home/caribu/Zotero/storage/F7IYLFS3/Ren et al. - 2022 - Global guidelines for breast cancer screening A systematic review.pdf;/home/caribu/Zotero/storage/NCID2NZ5/S0960977622000765.html}
}

@unpublished{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2021-07-11},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/5PXBYXCD/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{rodriguez-ruizStandAloneArtificialIntelligence2019,
  title = {Stand-{{Alone Artificial Intelligence}} for {{Breast Cancer Detection}} in {{Mammography}}: {{Comparison With}} 101 {{Radiologists}}},
  shorttitle = {Stand-{{Alone Artificial Intelligence}} for {{Breast Cancer Detection}} in {{Mammography}}},
  author = {Rodriguez-Ruiz, Alejandro and Lång, Kristina and Gubern-Merida, Albert and Broeders, Mireille and Gennaro, Gisella and Clauser, Paola and Helbich, Thomas H and Chevalier, Margarita and Tan, Tao and Mertelmeier, Thomas and Wallis, Matthew G and Andersson, Ingvar and Zackrisson, Sophia and Mann, Ritse M and Sechopoulos, Ioannis},
  date = {2019-03-05},
  journaltitle = {JNCI Journal of the National Cancer Institute},
  shortjournal = {J Natl Cancer Inst},
  volume = {111},
  number = {9},
  eprint = {30834436},
  eprinttype = {pubmed},
  pages = {916--922},
  issn = {0027-8874},
  doi = {10.1093/jnci/djy222},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6748773/},
  urldate = {2021-05-12},
  abstract = {Background Artificial intelligence (AI) systems performing at radiologist-like levels in the evaluation of digital mammography (DM) would improve breast cancer screening accuracy and efficiency. We aimed to compare the stand-alone performance of an AI system to that of radiologists in detecting breast cancer in DM. Methods Nine multi-reader, multi-case study datasets previously used for different research purposes in seven countries were collected. Each dataset consisted of DM exams acquired with systems from four different vendors, multiple radiologists’ assessments per exam, and ground truth verified by histopathological analysis or follow-up, yielding a total of 2652 exams (653 malignant) and interpretations by 101 radiologists (28\,296 independent interpretations). An AI system analyzed these exams yielding a level of suspicion of cancer present between 1 and 10. The detection performance between the radiologists and the AI system was compared using a noninferiority null hypothesis at a margin of 0.05. Results The performance of the AI system was statistically noninferior to that of the average of the 101 radiologists. The AI system had a 0.840 (95\% confidence interval [CI] = 0.820 to 0.860) area under the ROC curve and the average of the radiologists was 0.814 (95\% CI = 0.787 to 0.841) (difference 95\% CI = −0.003 to 0.055). The AI system had an AUC higher than 61.4\% of the radiologists. Conclusions The evaluated AI system achieved a cancer detection accuracy comparable to an average breast radiologist in this retrospective setting. Although promising, the performance and impact of such a system in a screening setting needs further investigation.},
  pmcid = {PMC6748773},
  file = {/home/caribu/Zotero/storage/C2D8Y9W2/Rodriguez-Ruiz et al. - 2019 - Stand-Alone Artificial Intelligence for Breast Can.pdf}
}

@online{ronnebergerUNetConvolutionalNetworks2015a,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  eprint = {1505.04597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1505.04597},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2025-07-23},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/NGRYF3SD/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf;/home/caribu/Zotero/storage/4HRICMBR/1505.html}
}

@article{rozanecKnowledgeGraphbasedRich2022,
  title = {Knowledge Graph-Based Rich and Confidentiality Preserving {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Rožanec, Jože M. and Fortuna, Blaž and Mladenić, Dunja},
  date = {2022-05-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {81},
  pages = {91--102},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.11.015},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253521002414},
  urldate = {2025-08-10},
  abstract = {The paper proposes a novel architecture for explainable artificial intelligence based on semantic technologies and artificial intelligence. We tailor the architecture for the domain of demand forecasting and validate it on a real-world case study. The explanations provided result from knowledge fusion regarding concepts describing features relevant to a particular forecast, related media events, and metadata regarding external datasets of interest. The Knowledge Graph enhances the quality of explanations by informing concepts at a higher abstraction level rather than specific features. By doing so, explanations avoid exposing sensitive details regarding the demand forecasting models, thus preserving confidentiality. In addition, the Knowledge Graph enables linking domain knowledge, forecasted values, and forecast explanations while also providing insights into actionable aspects on which users can take action. The ontology and dataset we developed for this use case are publicly available for further research.},
  keywords = {Confidentiality,Demand forecasting,Explainable Artificial Intelligence,Knowledge Graph,Privacy,Smart manufacturing},
  file = {/home/caribu/Zotero/storage/JEQW6SYG/S1566253521002414.html}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  date = {2019-05-13},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://www.nature.com/articles/s42256-019-0048-x},
  urldate = {2025-08-05},
  abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explainblack box models, rather than creating models that are interpretablein the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward – it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/UR75WHTQ/Rudin - 2019 - Stop explaining black box machine learning models for high stakes decisions and use interpretable mo.pdf}
}

@article{samuelStudiesMachineLearning1959,
  title = {Some {{Studies}} in {{Machine Learning Using}} the {{Game}} of {{Checkers}}},
  author = {Samuel, A. L.},
  date = {1959-07},
  journaltitle = {IBM Journal of Research and Development},
  volume = {3},
  number = {3},
  pages = {210--229},
  issn = {0018-8646},
  doi = {10.1147/rd.33.0210},
  url = {https://ieeexplore.ieee.org/abstract/document/5392560},
  urldate = {2025-08-11},
  abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
  file = {/home/caribu/Zotero/storage/4QJKRKFN/Samuel - 1959 - Some Studies in Machine Learning Using the Game of Checkers.pdf}
}

@inproceedings{sandlerMobileNetV2InvertedResiduals2018,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  date = {2018-06},
  pages = {4510--4520},
  publisher = {IEEE},
  location = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00474},
  url = {https://ieeexplore.ieee.org/document/8578572/},
  urldate = {2024-12-02},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/home/caribu/Zotero/storage/DYBHYQFR/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf}
}

@dataset{sawyer-leeCuratedBreastImaging2016,
  title = {Curated {{Breast Imaging Subset}} of {{Digital Database}} for {{Screening Mammography}} ({{CBIS-DDSM}})},
  author = {Sawyer-Lee, Rebecca and Gimenez, Francisco and Hoogi, Assaf and Rubin, Daniel},
  namea = {TCIA Team},
  nameatype = {collaborator},
  date = {2016},
  publisher = {The Cancer Imaging Archive},
  doi = {10.7937/K9/TCIA.2016.7O02S9CY},
  url = {https://wiki.cancerimagingarchive.net/x/lZNXAQ},
  urldate = {2023-03-09},
  abstract = {This CBIS-DDSM (Curated Breast Imaging Subset of DDSM) is an updated and standardized version of the Digital Database for Screening Mammography (DDSM). The DDSM is a database of 2,620 scanned film mammography studies. It contains normal, benign, and malignant cases with verified pathology information. The scale of the database along with ground truth validation makes the DDSM a useful tool in the development and testing of decision support systems. The CBIS-DDSM collection includes a subset of the DDSM data selected and curated by a trained mammographer. The images have been decompressed and converted to DICOM format. Updated ROI segmentation and bounding boxes, and pathologic diagnosis for training data are also included. Published research results from work in developing decision support systems in mammography are difficult to replicate due to the lack of a standard evaluation data set; most computer-aided diagnosis (CADx) and detection (CADe) algorithms for breast cancer in mammography are evaluated on private data sets or on unspecified subsets of public databases. Few well-curated public datasets have been provided for the mammography community. These include the DDSM, the Mammographic Imaging Analysis Society (MIAS) database, and the Image Retrieval in Medical Applications (IRMA) project. Although these public data sets are useful, they are limited in terms of data set size and accessibility. For example, most researchers using the DDSM do not leverage all its images for a variety of historical reasons. When the database was released in 1997, computational resources to process hundreds or thousands of images were not widely available. Additionally, the DDSM images are saved in non-standard compression files that require the use of decompression code that has not been updated or maintained for modern computers. Finally, the ROI annotations for the abnormalities in the DDSM were provided to indicate a general position of lesions, but not a precise segmentation for them. Therefore, many researchers must implement segmentation algorithms for accurate feature extraction. This causes an inability to directly compare the performance of methods or to replicate prior results. The CBIS-DDSM collection addresses that challenge by publicly releasing an curated and standardized version of the DDSM for evaluation of future CADx and CADe systems (sometimes referred to generally as CAD) research in mammography.},
  version = {1}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2020-02},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  url = {http://arxiv.org/abs/1610.02391},
  urldate = {2024-03-11},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/caribu/Zotero/storage/JIFZJSMZ/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf;/home/caribu/Zotero/storage/G4L5WVZX/1610.html}
}

@article{shenDeepLearningImprove2019a,
  title = {Deep {{Learning}} to {{Improve Breast Cancer Detection}} on {{Screening Mammography}}},
  author = {Shen, Li and Margolies, Laurie R. and Rothstein, Joseph H. and Fluder, Eugene and McBride, Russell and Sieh, Weiva},
  date = {2019-08-29},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {9},
  number = {1},
  pages = {12495},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-48995-4},
  url = {https://www.nature.com/articles/s41598-019-48995-4},
  urldate = {2025-08-10},
  abstract = {The rapid development of deep learning, a family of machine learning techniques, has spurred much interest in its application to medical imaging problems. Here, we develop a deep learning algorithm that can accurately detect breast cancer on screening mammograms using an “end-to-end” training approach that efficiently leverages training datasets with either complete clinical annotation or only the cancer status (label) of the whole image. In this approach, lesion annotations are required only in the initial training stage, and subsequent stages require only image-level labels, eliminating the reliance on rarely available lesion annotations. Our all convolutional network method for classifying screening mammograms attained excellent performance in comparison with previous methods. On an independent test set of digitized film mammograms from the Digital Database for Screening Mammography (CBIS-DDSM), the best single model achieved a per-image AUC of 0.88, and four-model averaging improved the AUC to 0.91 (sensitivity: 86.1\%, specificity: 80.1\%). On an independent test set of full-field digital mammography (FFDM) images from the INbreast database, the best single model achieved a per-image AUC of 0.95, and four-model averaging improved the AUC to 0.98 (sensitivity: 86.7\%, specificity: 96.1\%). We also demonstrate that a whole image classifier trained using our end-to-end approach on the CBIS-DDSM digitized film mammograms can be transferred to INbreast FFDM images using only a subset of the INbreast data for fine-tuning and without further reliance on the availability of lesion annotations. These findings show that automatic deep learning methods can be readily trained to attain high accuracy on heterogeneous mammography platforms, and hold tremendous promise for improving clinical tools to reduce false positive and false negative screening mammography results. Code and model available at: https://github.com/lishen/end2end-all-conv.},
  langid = {english},
  keywords = {Computational science,Computer science,Predictive markers,Software},
  file = {/home/caribu/Zotero/storage/UHK42W5G/Shen et al. - 2019 - Deep Learning to Improve Breast Cancer Detection on Screening Mammography.pdf}
}

@book{sickles2013acr,
  title = {{{ACR BI-RADS}}® {{Atlas}}, {{Breast Imaging Reporting}} and {{Data System}}},
  author = {Sickles, Edward A and D’Orsi, Carl J and Mendelson, Ellen B and Morris, Elizabeth A},
  date = {2013},
  edition = {5},
  publisher = {American College of Radiology},
  location = {Reston, VA},
  isbn = {978-1-55903-016-8},
  pagetotal = {689}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2023-11-10},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {prepublished},
  version = {6},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/MECU8ZYU/Simonyan y Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/home/caribu/Zotero/storage/6JDQI4G8/1409.html}
}

@article{smithDelaySurgicalTreatment2013,
  title = {Delay in {{Surgical Treatment}} and {{Survival After Breast Cancer Diagnosis}} in {{Young Women}} by {{Race}}/{{Ethnicity}}},
  author = {Smith, Erlyn C. and Ziogas, Argyrios and Anton-Culver, Hoda},
  date = {2013-06-01},
  journaltitle = {JAMA Surgery},
  shortjournal = {JAMA Surg},
  volume = {148},
  number = {6},
  pages = {516},
  issn = {2168-6254},
  doi = {10.1001/jamasurg.2013.1680},
  url = {http://archsurg.jamanetwork.com/article.aspx?doi=10.1001/jamasurg.2013.1680},
  urldate = {2021-05-19},
  langid = {english},
  file = {/home/caribu/Zotero/storage/FMNA72VR/Smith et al. - 2013 - Delay in Surgical Treatment and Survival After Bre.pdf}
}

@article{smolarzBreastCancerEpidemiology2022,
  title = {Breast {{Cancer}}—{{Epidemiology}}, {{Classification}}, {{Pathogenesis}} and {{Treatment}} ({{Review}} of {{Literature}})},
  author = {Smolarz, Beata and Nowak, Anna Zadrożna and Romanowicz, Hanna},
  date = {2022-05-23},
  journaltitle = {Cancers},
  shortjournal = {Cancers},
  volume = {14},
  number = {10},
  pages = {2569},
  issn = {2072-6694},
  doi = {10.3390/cancers14102569},
  url = {https://www.mdpi.com/2072-6694/14/10/2569},
  urldate = {2024-07-29},
  abstract = {Breast cancer is the most-commonly diagnosed malignant tumor in women in the world, as well as the first cause of death from malignant tumors. The incidence of breast cancer is constantly increasing in all regions of the world. For this reason, despite the progress in its detection and treatment, which translates into improved mortality rates, it seems necessary to look for new therapeutic methods, and predictive and prognostic factors. Treatment strategies vary depending on the molecular subtype. Breast cancer treatment is multidisciplinary; it includes approaches to locoregional therapy (surgery and radiation therapy) and systemic therapy. Systemic therapies include hormone therapy for hormone-positive disease, chemotherapy, anti-HER2 therapy for HER2-positive disease, and quite recently, immunotherapy. Triple negative breast cancer is responsible for more than 15–20\% of all breast cancers. It is of particular research interest as it presents a therapeutic challenge, mainly due to its low response to treatment and its highly invasive nature. Future therapeutic concepts for breast cancer aim to individualize therapy and de-escalate and escalate treatment based on cancer biology and early response to therapy. The article presents a review of the literature on breast carcinoma—a disease affecting women in the world.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/QK52V2IM/Smolarz et al. - 2022 - Breast Cancer—Epidemiology, Classification, Pathog.pdf}
}

@dataset{sucklingMammographicImageAnalysis2015,
  title = {Mammographic {{Image Analysis Society}} ({{MIAS}}) Database v1.21},
  author = {Suckling, John and Parker, J. and Dance, D. and Astley, S. and Hutt, I. and Boggis, C. and Ricketts, I. and Stamatakis, E. and Cerneaz, N. and Kok, S. and Taylor, P. and Betal, D. and Savage, J.},
  date = {2015-08-28},
  doi = {10/250394},
  url = {https://www.repository.cam.ac.uk/handle/1810/250394},
  urldate = {2023-03-23},
  abstract = {The Mammographic Image Analysis Society database of digital mammograms (v1.21). Contains the original 322 images (161 pairs) at 50 micron resolution in "Portable Gray Map" (PGM) format and associated truth data.},
  langid = {english},
  annotation = {Accepted: 2015-08-28T11:57:00Z},
  file = {/home/caribu/Zotero/storage/Q5XSY26I/Suckling et al. - 2015 - Mammographic Image Analysis Society (MIAS) databas.pdf}
}

@article{suhAutomatedBreastCancer2020,
  title = {Automated {{Breast Cancer Detection}} in {{Digital Mammograms}} of {{Various Densities}} via {{Deep Learning}}},
  author = {Suh, Yong Joon and Jung, Jaewon and Cho, Bum-Joo},
  date = {2020-11-06},
  journaltitle = {Journal of Personalized Medicine},
  shortjournal = {JPM},
  volume = {10},
  number = {4},
  pages = {211},
  issn = {2075-4426},
  doi = {10.3390/jpm10040211},
  url = {https://www.mdpi.com/2075-4426/10/4/211},
  urldate = {2021-07-11},
  abstract = {Mammography plays an important role in screening breast cancer among females, and artificial intelligence has enabled the automated detection of diseases on medical images. This study aimed to develop a deep learning model detecting breast cancer in digital mammograms of various densities and to evaluate the model performance compared to previous studies. From 1501 subjects who underwent digital mammography between February 2007 and May 2015, craniocaudal and mediolateral view mammograms were included and concatenated for each breast, ultimately producing 3002 merged images. Two convolutional neural networks were trained to detect any malignant lesion on the merged images. The performances were tested using 301 merged images from 284 subjects and compared to a meta-analysis including 12 previous deep learning studies. The mean area under the receiver-operating characteristic curve (AUC) for detecting breast cancer in each merged mammogram was 0.952 ± 0.005 by DenseNet-169 and 0.954 ± 0.020 by EfficientNet-B5, respectively. The performance for malignancy detection decreased as breast density increased (density A, mean AUC = 0.984 vs. density D, mean AUC = 0.902 by DenseNet-169). When patients’ age was used as a covariate for malignancy detection, the performance showed little change (mean AUC, 0.953 ± 0.005). The mean sensitivity and specificity of the DenseNet-169 (87 and 88\%, respectively) surpassed the mean values (81 and 82\%, respectively) obtained in a meta-analysis. Deep learning would work efficiently in screening breast cancer in digital mammograms of various densities, which could be maximized in breasts with lower parenchyma density.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/TEHN45M8/Suh et al. - 2020 - Automated Breast Cancer Detection in Digital Mammo.pdf}
}

@online{szegedyInceptionv4InceptionResNetImpact2016,
  title = {Inception-v4, {{Inception-ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
  date = {2016-08-23},
  eprint = {1602.07261},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.07261},
  url = {http://arxiv.org/abs/1602.07261},
  urldate = {2025-03-20},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/2WCHFNSC/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf}
}

@article{tabarInsightsBreastCancer2015,
  title = {Insights from the {{Breast Cancer Screening Trials}}: {{How Screening Affects}} the {{Natural History}} of {{Breast Cancer}} and {{Implications}} for {{Evaluating Service Screening Programs}}},
  shorttitle = {Insights from the {{Breast Cancer Screening Trials}}},
  author = {Tabár, László and Yen, Amy Ming-Fang and Wu, Wendy Yi-Ying and Chen, Sam Li-Sheng and Chiu, Sherry Yueh-Hsia and Fann, Jean Ching-Yuan and Ku, May Mei-Sheng and Smith, Robert A and Duffy, Stephen W and Chen, Tony Hsiu-Hsi},
  date = {2015-01},
  journaltitle = {The Breast Journal},
  shortjournal = {Breast J},
  volume = {21},
  number = {1},
  pages = {13--20},
  issn = {1075122X},
  doi = {10.1111/tbj.12354},
  url = {http://doi.wiley.com/10.1111/tbj.12354},
  urldate = {2021-05-19},
  langid = {english},
  file = {/home/caribu/Zotero/storage/3FSUBGGN/Tabár et al. - 2015 - Insights from the Breast Cancer Screening Trials .pdf}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  date = {2019-05-24},
  pages = {6105--6114},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/tan19a.html},
  urldate = {2024-10-11},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/caribu/Zotero/storage/XEIILKUM/Tan y Le - 2019 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf}
}

@online{tanEfficientNetV2SmallerModels2021,
  title = {{{EfficientNetV2}}: {{Smaller Models}} and {{Faster Training}}},
  shorttitle = {{{EfficientNetV2}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  date = {2021-06-23},
  eprint = {2104.00298},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.00298},
  urldate = {2023-11-09},
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop these models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/RPVC7ZZM/Tan y Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf}
}

@online{tanPredictingLongTerm2023,
  title = {Predicting Long Term Breast Cancer Risk Using Longitudinal Mammographic Screening History},
  author = {Tan, Tao and Wang, Xin and Gao, Yuan and Su, Ruisheng and Zhang, Tianyu and Han, Luyi and Teuwen, Jonas and Kroes, Jaap and D'Angelo, Anna and Drukker, Caroline and Schmidt, Marjanka and Beets-Tan, Regina and Karssemeijer, Nico and Mann, Ritse},
  date = {2023-09-27},
  eprinttype = {In Review},
  doi = {10.21203/rs.3.rs-3376534/v1},
  url = {https://www.researchsquare.com/article/rs-3376534/v1},
  urldate = {2025-06-13},
  abstract = {Abstract           Risk assessment of breast cancer (BC) seeks to enhance individualized screening and prevention strategies. Recent deep learning (DL) risk models based on mammography have shown superiority in short-term risk prediction compared to traditional risk factor-based models. However, these models primarily rely on single-time exams, which emphasize the detection of existing lesions and may ignore the temporal changes in breast tissues. In this study, we present the Multi-Time Point Breast Cancer Risk Model (MTP-BCR), a novel temporospatial DL risk model that integrates traditional BC risk factors and longitudinal mammography data to identify subtle changes in breast tissue indicative of future malignancy. Utilizing a large inhouse dataset comprising risk factors and 171,168 mammograms involving 9,133 women, we evaluate the performance of the MTP-BCR model in long-term risk prediction. Our model demonstrates a significant improvement in 10-year risk prediction with an area under the receiver operating characteristics (AUC) of 0.80, outperforming the traditional BCSC 10-year risk model, our pure image model (without risk factors), and is also superior to other SOTA methods at 5-year AUC in various screening cohorts. Furthermore, MTP-BCR provides unilateral breast-level predictions with AUCs up to 0.81 and 0.77 for 5-year and 10-year risk assessments, respectively. External validation in the public CSAW-CC dataset demonstrates the consistent advantage of our multi-time point-based model compared to the single-time point-based method. For the prediction of breast cancer recurrence, our MTP-BCR obtains a 5-year AUC of 0.71 which also surpasses other methods. The heatmaps derived from our model may help clinicians better understand the progression from normal tissue to cancerous growth, enhancing interpretability in breast cancer risk assessment.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/caribu/Zotero/storage/YGR3WRHQ/Tan et al. - 2023 - Predicting long term breast cancer risk using longitudinal mammographic screening history.pdf}
}

@article{varoquauxMachineLearningMedical2022,
  title = {Machine Learning for Medical Imaging: Methodological Failures and Recommendations for the Future},
  shorttitle = {Machine Learning for Medical Imaging},
  author = {Varoquaux, Gaël and Cheplygina, Veronika},
  date = {2022-04-12},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume = {5},
  number = {1},
  pages = {48},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00592-y},
  url = {https://www.nature.com/articles/s41746-022-00592-y},
  urldate = {2025-08-11},
  abstract = {Research in computer analysis of medical images bears many promises to improve patients’ health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.},
  langid = {english},
  keywords = {Computer science,Medical research,Research data},
  file = {/home/caribu/Zotero/storage/V8GN2S4Z/Varoquaux y Cheplygina - 2022 - Machine learning for medical imaging methodological failures and recommendations for the future.pdf}
}

@online{wickstromRELAXRepresentationLearning2022,
  title = {{{RELAX}}: {{Representation Learning Explainability}}},
  shorttitle = {{{RELAX}}},
  author = {Wickstrøm, Kristoffer K. and Trosten, Daniel J. and Løkse, Sigurd and Boubekki, Ahcène and Mikalsen, Karl Øyvind and Kampffmeyer, Michael C. and Jenssen, Robert},
  date = {2022-02-21},
  eprint = {2112.10161},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2112.10161},
  urldate = {2023-11-27},
  abstract = {Despite the significant improvements that self-supervised representation learning has led to when learning from unlabeled data, no methods have been developed that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the representation space between an input and masked out versions of itself, providing intuitive explanations and significantly outperforming the gradient-based baselines. We provide theoretical interpretations of RELAX and conduct a novel analysis of feature extractors trained using supervised and unsupervised learning, providing insights into different learning strategies. Moreover, we conduct a user study to assess how well the proposed approach aligns with human intuition and show that the proposed method outperforms the baselines in both the quantitative and human evaluation studies. Finally, we illustrate the usability of RELAX in several use cases and highlight that incorporating uncertainty can be essential for providing faithful explanations, taking a crucial step towards explaining representations.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/caribu/Zotero/storage/PLHY4CTD/Wickstrøm et al. - 2022 - RELAX Representation Learning Explainability.pdf}
}

@article{wuDeepNeuralNetworks2020,
  title = {Deep {{Neural Networks Improve Radiologists}}’ {{Performance}} in {{Breast Cancer Screening}}},
  author = {Wu, Nan and Phang, Jason and Park, Jungkyu and Shen, Yiqiu and Huang, Zhe and Zorin, Masha and Jastrzębski, Stanisław and Févry, Thibault and Katsnelson, Joe and Kim, Eric and Wolfson, Stacey and Parikh, Ujas and Gaddam, Sushma and Lin, Leng Leng Young and Ho, Kara and Weinstein, Joshua D. and Reig, Beatriu and Gao, Yiming and Toth, Hildegard and Pysarenko, Kristine and Lewin, Alana and Lee, Jiyon and Airola, Krystal and Mema, Eralda and Chung, Stephanie and Hwang, Esther and Samreen, Naziya and Kim, S. Gene and Heacock, Laura and Moy, Linda and Cho, Kyunghyun and Geras, Krzysztof J.},
  date = {2020-04},
  journaltitle = {IEEE Transactions on Medical Imaging},
  volume = {39},
  number = {4},
  pages = {1184--1194},
  issn = {1558-254X},
  doi = {10.1109/TMI.2019.2945514},
  url = {https://ieeexplore.ieee.org/document/8861376},
  urldate = {2025-08-10},
  abstract = {We present a deep convolutional neural network for breast cancer screening exam classification, trained, and evaluated on over 200000 exams (over 1000000 images). Our network achieves an AUC of 0.895 in predicting the presence of cancer in the breast, when tested on the screening population. We attribute the high accuracy to a few technical advances. 1) Our network’s novel two-stage architecture and training procedure, which allows us to use a high-capacity patch-level network to learn from pixel-level labels alongside a network learning from macroscopic breast-level labels. 2) A custom ResNet-based network used as a building block of our model, whose balance of depth and width is optimized for high-resolution medical images. 3) Pretraining the network on screening BI-RADS classification, a related task with more noisy labels. 4) Combining multiple input views in an optimal way among a number of possible choices. To validate our model, we conducted a reader study with 14 readers, each reading 720 screening mammogram exams, and show that our model is as accurate as experienced radiologists when presented with the same data. We also show that a hybrid model, averaging the probability of malignancy predicted by a radiologist with a prediction of our neural network, is more accurate than either of the two separately. To further understand our results, we conduct a thorough analysis of our network’s performance on different subpopulations of the screening population, the model’s design, training procedure, errors, and properties of its internal representations. Our best models are publicly available at https://github.com/nyukat/breast\_cancer\_classifier.},
  keywords = {Biomedical imaging,Breast cancer,breast cancer screening,deep convolutional neural networks,Deep learning,mammography,Predictive models,Task analysis,Training},
  file = {/home/caribu/Zotero/storage/TFABZYV2/Wu et al. - 2020 - Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening.pdf}
}

@article{yalaRobustMammographybasedModels2021,
  title = {Toward Robust Mammography-Based Models for Breast Cancer Risk},
  author = {Yala, Adam and Mikhael, Peter G. and Strand, Fredrik and Lin, Gigin and Smith, Kevin and Wan, Yung-Liang and Lamb, Leslie and Hughes, Kevin and Lehman, Constance and Barzilay, Regina},
  date = {2021-01-27},
  journaltitle = {Science Translational Medicine},
  shortjournal = {Sci. Transl. Med.},
  volume = {13},
  number = {578},
  pages = {eaba4373},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aba4373},
  url = {https://www.science.org/doi/10.1126/scitranslmed.aba4373},
  urldate = {2025-06-18},
  abstract = {An algorithm to predict breast cancer risk, Mirai, outperforms clinical risk models across test cohorts from the United States, Sweden, and Taiwan.           ,              Don’t risk it                            Mammograms are a common but imperfect way of assessing breast cancer risk. Current U.S. breast cancer screening guidelines all use a component of cancer risk assessment to inform clinical course. Yala               et al.               developed a machine learning model called “Mirai” to predict breast cancer risk based on traditional mammograms. The authors’ risk model performed better than Tyrer-Cuzick and previous deep learning models at identifying both 5-year breast cancer risk and high-risk patients across multiple international cohorts. Mirai also performed similarly across race and ethnicity categories, suggesting the potential for improvement in patient care across the board.                        ,                             Improved breast cancer risk models enable targeted screening strategies that achieve earlier detection and less screening harm than existing guidelines. To bring deep learning risk models to clinical practice, we need to further refine their accuracy, validate them across diverse populations, and demonstrate their potential to improve clinical workflows. We developed Mirai, a mammography-based deep learning model designed to predict risk at multiple timepoints, leverage potentially missing risk factor information, and produce predictions that are consistent across mammography machines. Mirai was trained on a large dataset from Massachusetts General Hospital (MGH) in the United States and tested on held-out test sets from MGH, Karolinska University Hospital in Sweden, and Chang Gung Memorial Hospital (CGMH) in Taiwan, obtaining C-indices of 0.76 (95\% confidence interval, 0.74 to 0.80), 0.81 (0.79 to 0.82), and 0.79 (0.79 to 0.83), respectively. Mirai obtained significantly higher 5-year ROC AUCs than the Tyrer-Cuzick model (                                P                              {$<$} 0.001) and prior deep learning models Hybrid DL (                                P                              {$<$} 0.001) and Image-Only DL (                                P                              {$<$} 0.001), trained on the same dataset. Mirai more accurately identified high-risk patients than prior methods across all datasets. On the MGH test set, 41.5\% (34.4 to 48.5) of patients who would develop cancer within 5 years were identified as high risk, compared with 36.1\% (29.1 to 42.9) by Hybrid DL (                                P                              = 0.02) and 22.9\% (15.9 to 29.6) by the Tyrer-Cuzick model (                                P                              {$<$} 0.001).},
  langid = {english},
  file = {/home/caribu/Zotero/storage/EJDHF68F/Yala et al. - 2021 - Toward robust mammography-based models for breast cancer risk.pdf}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  pages = {818--833},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image},
  file = {/home/caribu/Zotero/storage/A9I2JVSH/Zeiler y Fergus - 2014 - Visualizing and understanding convolutional networ.pdf;/home/caribu/Zotero/storage/QDMLR3ID/Zeiler y Fergus - 2014 - Visualizing and Understanding Convolutional Networks.pdf}
}

@online{zhangTopdownNeuralAttention2016,
  title = {Top-down {{Neural Attention}} by {{Excitation Backprop}}},
  author = {Zhang, Jianming and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  date = {2016-08-01},
  eprint = {1608.00507},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1608.00507},
  urldate = {2024-08-01},
  abstract = {We aim to model the top-down attention of a Convolutional Neural Network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. In experiments, we demonstrate the accuracy and generalizability of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/caribu/Zotero/storage/6V3FLGWQ/Zhang et al. - 2016 - Top-down Neural Attention by Excitation Backprop.pdf}
}

@article{zhengOverviewArtificialIntelligence2023,
  title = {Overview of {{Artificial Intelligence}} in {{Breast Cancer Medical Imaging}}},
  author = {Zheng, Dan and He, Xiujing and Jing, Jing},
  date = {2023-01-04},
  journaltitle = {Journal of Clinical Medicine},
  shortjournal = {JCM},
  volume = {12},
  number = {2},
  pages = {419},
  issn = {2077-0383},
  doi = {10.3390/jcm12020419},
  url = {https://www.mdpi.com/2077-0383/12/2/419},
  urldate = {2024-05-09},
  abstract = {The heavy global burden and mortality of breast cancer emphasize the importance of early diagnosis and treatment. Imaging detection is one of the main tools used in clinical practice for screening, diagnosis, and treatment efficacy evaluation, and can visualize changes in tumor size and texture before and after treatment. The overwhelming number of images, which lead to a heavy workload for radiologists and a sluggish reporting period, suggests the need for computer-aid detection techniques and platform. In addition, complex and changeable image features, heterogeneous quality of images, and inconsistent interpretation by different radiologists and medical institutions constitute the primary difficulties in breast cancer screening and imaging diagnosis. The advancement of imaging-based artificial intelligence (AI)-assisted tumor diagnosis is an ideal strategy for improving imaging diagnosis efficient and accuracy. By learning from image data input and constructing algorithm models, AI is able to recognize, segment, and diagnose tumor lesion automatically, showing promising application prospects. Furthermore, the rapid advancement of “omics” promotes a deeper and more comprehensive recognition of the nature of cancer. The fascinating relationship between tumor image and molecular characteristics has attracted attention to the radiomic and radiogenomics, which allow us to perform analysis and detection on the molecular level with no need for invasive operations. In this review, we integrate the current developments in AI-assisted imaging diagnosis and discuss the advances of AI-based breast cancer precise diagnosis from a clinical point of view. Although AI-assisted imaging breast cancer screening and detection is an emerging field and draws much attention, the clinical application of AI in tumor lesion recognition, segmentation, and diagnosis is still limited to research or in limited patients’ cohort. Randomized clinical trials based on large and high-quality cohort are lacking. This review aims to describe the progress of the imaging-based AI application in breast cancer screening and diagnosis for clinicians.},
  langid = {english},
  file = {/home/caribu/Zotero/storage/IPVP72UB/Zheng et al. - 2023 - Overview of Artificial Intelligence in Breast Canc.pdf}
}

@inproceedings{zhouLearningDeepFeatures2016a,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2016-06},
  pages = {2921--2929},
  publisher = {IEEE},
  location = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.319},
  url = {http://ieeexplore.ieee.org/document/7780688/},
  urldate = {2025-06-07},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/home/caribu/Zotero/storage/CUEY3VGI/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localization.pdf}
}
